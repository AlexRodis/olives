{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84ec93-37d9-4fc2-be99-10340d580512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Βιβλιοθήκες\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gekko as gk\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import typing\n",
    "import pymc\n",
    "import itertools\n",
    "import math\n",
    "import functools\n",
    "from pca import pca\n",
    "from IPython.display import display, HTML\n",
    "import prince\n",
    "import networkx\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a3abf-6515-4a6b-a8fe-b576dc73f560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Γενικές επιλογές εμφάνισης\n",
    "pd.set_option('display.max_rows',10)\n",
    "pd.set_option('display.max_columns',30)\n",
    "sns.set(rc={'figure.figsize':(14,9)})\n",
    "sns.set(font_scale = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf98f2a-8911-4e6b-b6c0-a3ed02939255",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scale = lambda df: pd.DataFrame(data = StandardScaler().fit_transform(df), columns = df.columns, index=df.index)\n",
    "\n",
    "def render_df(df:pd.DataFrame):\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(df.to_html()))\n",
    "    return\n",
    "\n",
    "def full_display(r=None, c=None):\n",
    "    pd.set_option('display.max_rows', r)\n",
    "    pd.set_option('display.max_columns',c)\n",
    "    return\n",
    "\n",
    "def reset_display():\n",
    "    pd.set_option('display.max_rows',10)\n",
    "    pd.set_option('display.max_columns',30)\n",
    "\n",
    "def full_display_once(df:pd.DataFrame, r=None, c=None):\n",
    "    full_display(r=r, c=c)\n",
    "    render_df(df)\n",
    "    reset_display()\n",
    "\n",
    "def rowwise_value_counts(df:pd.DataFrame):\n",
    "    vcounts_df = pd.DataFrame(data = df.apply(lambda x: x.value_counts()).T.stack()).astype(int).T\n",
    "    vcounts_df.index = ['']\n",
    "    return vcounts_df\n",
    "\n",
    "invert_dict = lambda e: {v:k for k, v in e.items()}\n",
    "\n",
    "def categorical_scatter(X:pd.DataFrame, Y:typing.Optional[pd.DataFrame],cols:int=3,max_rows:int = 3,\n",
    "                             figsize:tuple[int, int] = (30, 15), xaxis_label_size: int=12,\n",
    "                             yaxis_label_size:int=12, categorical:typing.Optional[str]='hue')->typing.Optional[plt.figure]:\n",
    "    '''\n",
    "        Generate pair-wise scatter plots for a given DataFrame, with optional support for large Datasets\n",
    "        and categorical variables. Yields a figure with `max_rows x cols` scatterplots per call. When\n",
    "        `categorical=None` only generates pair-wise scatterplots for the columns of X, else generates all\n",
    "        combinations of X-column pair and Y columns. When `categorical='hue'` values of the categorical Y\n",
    "        variable will be depicted color-coded and when `categorical=size` different factors of the Y variable\n",
    "        will have different sizes instead.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "            - X:pandas.DataFrame := The data to depict. When Y is also specified, X is assumed to be the DataFrame\n",
    "            of indicator variables\n",
    "            \n",
    "            - Y:Optional[pandas.DataFrame] := Optional Dataframe of categorical variables to depict. Ignored if \n",
    "            `category` is `None`.\n",
    "            \n",
    "            - idx_lvl:int=1 := For multilevel indexed dataframes the lowest level to squash the index on. Must be non-negative\n",
    "            \n",
    "            - cols:int=3 := Number of columns for the resulting facet grid plot. Must be non-negative. Defaults to 3\n",
    "            \n",
    "            - max_rows:int=3 := Maximum number of rows per batch the generator yields. Defaults to 3 and must be non-negative\n",
    "            \n",
    "            - figsize:tuple[int, int] := A `height x width` tuple for the generated plots. Defaults to `(30, 15)`\n",
    "            \n",
    "            - xaxis_label_size:int=12 := The size of the x-axis titles for each subplot. Must be non-negative and defaults to 12\n",
    "            \n",
    "            - yaxis_label_size:int=12 := The size of the y-axis titles for each subplot. Must be non-negative and defaults to 12\n",
    "            \n",
    "            - categorical:Optional[str] := Set the display of the categorical variable. (1) `None` ignores `Y` and only displays X\n",
    "            pair-wise scatterplots, (2) 'hue' displays the categorical variable of `Y` as color and (3) 'size' displays\n",
    "            the categorical variable with differently sized points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        \n",
    "            - fig:matplotlib.pyplot.figure := The figure object, a FacetPlot of scatterplots\n",
    "            \n",
    "       \n",
    "       Raises:\n",
    "        ------\n",
    "        \n",
    "            - WIP\n",
    "            \n",
    "            - ValueError \n",
    "    '''\n",
    "    XY = pd.concat([Y, X], axis=1)\n",
    "    idx_lvl= XY.columns.nlevels-1\n",
    "    skip_multiindex = lambda df,i , idx_level=idx_lvl: df.columns[i][idx_level] if idx_level else df.columns[i]\n",
    "    if idx_lvl:\n",
    "        XY.columns = XY.columns.get_level_values(idx_lvl)\n",
    "    x_combs = math.comb(X.shape[1], 2)\n",
    "    MAX_ROWS = max_rows\n",
    "    \n",
    "    if categorical is not None:\n",
    "        plots = x_combs*Y.shape[1]\n",
    "    else:\n",
    "        plots = x_combs\n",
    "    ncols = cols\n",
    "    size_scaling_factor = None\n",
    "    total_rows = math.ceil(plots/ncols)\n",
    "    total_figures = math.ceil(total_rows/MAX_ROWS)\n",
    "    X_pairs = itertools.combinations(range(X.shape[1]), 2)\n",
    "    subplots = itertools.product(X_pairs,range(Y.shape[1]) ) if categorical is not None else X_pairs\n",
    "    exhaustion_sentinel = False\n",
    "    while True:\n",
    "        if exhaustion_sentinel: break\n",
    "        fig, axs = plt.subplots(nrows=MAX_ROWS, ncols=ncols, figsize=figsize)\n",
    "        ax_indices = itertools.product(range(MAX_ROWS), range(ncols), repeat=1)\n",
    "        plotslice = itertools.islice(subplots, MAX_ROWS*ncols)\n",
    "        fig_generator = itertools.zip_longest( ax_indices, plotslice, fillvalue = ((None, None), None) )\n",
    "        for (axi, axj), e in fig_generator:\n",
    "            e = tuple(flatten(e))\n",
    "            if e[0] is not None:\n",
    "                if categorical == 'size':\n",
    "                    sns.scatterplot(x=skip_multiindex(chem_indicators, e[0]), y=skip_multiindex(chem_indicators, e[1]), data=XY,\n",
    "                       size=skip_multiindex(origin_targets, e[2]), ax=axs[axi, axj], legend=True)\n",
    "                elif categorical == 'hue':\n",
    "                    sns.scatterplot(x=skip_multiindex(chem_indicators, e[0]), y=skip_multiindex(chem_indicators, e[1]), data=XY,\n",
    "                       hue=skip_multiindex(origin_targets, e[2]), ax=axs[axi, axj], legend=True)\n",
    "                elif categorical is None:\n",
    "                    sns.scatterplot(x=skip_multiindex(chem_indicators, e[0]), y=skip_multiindex(chem_indicators, e[1]), data=XY,\n",
    "                       ax=axs[axi, axj], legend=True)\n",
    "                axs[axi, axj].set_xlabel(skip_multiindex(chem_indicators, e[0]))\n",
    "                axs[axi, axj].set_ylabel(skip_multiindex(chem_indicators, e[1]))\n",
    "                axs[axi, axj].xaxis.label.set_size(xaxis_label_size)\n",
    "                axs[axi, axj].yaxis.label.set_size(yaxis_label_size)\n",
    "                \n",
    "            else:\n",
    "                axs[axi, axj].axis('off')\n",
    "                exhaustion_sentinel = True\n",
    "        yield fig \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def flatten(xs):\n",
    "    from collections.abc import Iterable\n",
    "    for x in xs:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "def tidy_multiindex(df:pd.DataFrame, sep:str=\".\"):\n",
    "    '''\n",
    "        Compress a hierarchically indexed dataframe to standardized tidy\n",
    "        format. A unique sepperator `sep` is used to allow reversal. All\n",
    "        levels of the index are appended together with a delimeter to allow\n",
    "        reversals.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        \n",
    "            - df:pandas.DataFrame := A `pandas.DataFrame` hierarchically indexed\n",
    "            \n",
    "            - sep:str='_._' := A delimenter delineating the different levels\n",
    "            of the index. Ensure it is not present in any column name to avoid\n",
    "            a malformed index\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        \n",
    "            - ndf:pandas.DataFrame := The DataFrame with a single-level index\n",
    "    '''\n",
    "    tidy_cols = (functools.reduce(lambda e1,e2: str(e1)+sep+str(e2), col ) for col in df.columns)\n",
    "    ndf = df.copy(deep=True)\n",
    "    ndf.columns = tidy_cols\n",
    "    return ndf\n",
    "\n",
    "def reverse_tidy_multiindex(df:pd.DataFrame, sep=\".\"):\n",
    "    '''\n",
    "        Reverses the tidying to a hierachical format. Different\n",
    "        levels of the index are identified based on \"sep\"\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "            - df:pandas.DataFrame := The dataframe to process\n",
    "            \n",
    "            - sep:str='_._' := The string delimeter, sepperating\n",
    "            values for different levels of the index\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        \n",
    "            - ndf:pandas.DataFrame := The dataframe with hierarchical index\n",
    "    '''\n",
    "    h_cols = (tuple(col.split(sep)) for col in df.columns)\n",
    "    ndf = df.copy(deep=True)\n",
    "    ndf.columns = pd.MultiIndex.from_tuples(h_cols)\n",
    "    return ndf\n",
    "\n",
    "def undummify(df:pd.DataFrame,cols:list[str, tuple[str]],ncol_name:typing.Union[str,tuple[str]],\n",
    "              sep:typing.Optional[str]=None,\n",
    "              rmap:typing.Optional[dict[int, typing.Union[str, tuple[str]]]]=None\n",
    "             )->pd.DataFrame:\n",
    "    '''\n",
    "        Reverses hot-encoded variables in the DataFrame. A series of hot-encoded\n",
    "        variable levels $(i_1, i2, \\dots, i_k)$ is mapped to a single new column\n",
    "        $(k)$, whose name is specified by `ncol_name`, in the new dataframe. Pre\n",
    "        vious level columns are dropped.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        \n",
    "            - df:pandas.DataFrame := The DataFrame to operate upon\n",
    "            \n",
    "            - cols:list[str, tuple[str]] := A list of columns, representing the\n",
    "            levels of a categorical variable\n",
    "            \n",
    "            - sep:Optional[str] := sepperator for variable level. Currently ignored\n",
    "            \n",
    "            - ncol_name:Union[str, tuple[str]] := Name of the new categorical column\n",
    "            \n",
    "            - remap:Optional[dict[int, Union[str, tuple[str]]]] := A dictionary mapping\n",
    "            of categorical levels to values. Keys are the assumed to be levels, values\n",
    "            are assumed to be values (i.e. strings). When provided, the previous levels\n",
    "            will be replaced by the specified mappings in the new DataFrame\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        \n",
    "            - ndf:pandas.DataFrame := The processed dataframe\n",
    "     '''\n",
    "    _df = coredf.loc[:, cols]\n",
    "    for i, col in enumerate(cols, 1):\n",
    "        _df.loc[:, col] = i*_df.loc[:, col]\n",
    "    ndf = df.copy(deep=True)\n",
    "    ndf.drop(cols, axis=1, inplace=True)\n",
    "    ndf[ncol_name] = _df.max(axis=1)\n",
    "    c1 = df.columns.tolist()\n",
    "    i = c1.index(cols[0])\n",
    "    swp = ndf.columns.tolist()[:i-1]+[ndf.columns.tolist()[-1]]+ndf.columns.tolist()[i:-1]\n",
    "    ndf = ndf.loc[:, swp]\n",
    "    if rmap is not None:\n",
    "        ndf = ndf.replace(rmap)\n",
    "    return ndf\n",
    "list_difference = lambda l1, l2: [e for e  in l1 if e not in set(l2)]\n",
    "\n",
    "def corrspace_graph(df:pd.DataFrame):\n",
    "    '''\n",
    "        Generate a correlation graph representation of `df` datasets.\n",
    "        Every node in the resulting graph is a variable and every \n",
    "        vertex between two nodes represents the correlation between\n",
    "        the two variables, the correlation being the verted weight\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        \n",
    "            - df:pandas.DataFrame := A dataset to depict\n",
    "            \n",
    "        Returns:\n",
    "            - WIP\n",
    "    '''\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d7999-54af-42d0-82ce-66ee3f4f9a8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_model(Xtrain, Xtest, Ytrain, Ytest, loss:str='categorical_crossentropy',\n",
    "             optimizer:str='adadelta',\n",
    "             layers:typing.Optional[list[tuple[int, str]]]=None,\n",
    "             metrics=['accuracy'], summary:bool=True, epochs:int=10,\n",
    "             activation:str='sigmoid',batch:int=64, callbacks:list=[],\n",
    "            verbosity:int=1):\n",
    "    M = X.shape[1]\n",
    "    input_layer = tf.keras.Input(shape=(M,))\n",
    "    x = input_layer\n",
    "    for neurons, activation in layers:\n",
    "        x = tf.keras.layers.Dense(math.ceil(neurons), activation=activation)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(math.ceil(Ytrain.shape[1]), activation='sigmoid')(x)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=x)\n",
    "    model.compile(optimizer=optimizer, loss=loss, \n",
    "                metrics=metrics)\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    model.fit(Xtrain, Ytrain, epochs=epochs, validation_data = (Xtest, Ytest),\n",
    "              batch_size=batch, callbacks=callbacks, verbose=verbosity)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c9a31-5c0a-4e6f-bab4-6a8b0dd8c69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_autoencoder(Xtrain, Xtest, Ytrain, Ytest, loss:str='categorical_crossentropy',\n",
    "             optimizer:str='adadelta',\n",
    "             layers:typing.Optional[list[tuple[int, str]]]=None,\n",
    "             metrics=['accuracy'], summary:bool=True, epochs:int=10,\n",
    "             activation:str='sigmoid',batch:int=64 ):\n",
    "    M = X.shape[1]\n",
    "    input_layer = tf.keras.Input(shape=(M,))\n",
    "    x = input_layer\n",
    "    rev_layers = copy(layers)\n",
    "    rev_layers.reverse()\n",
    "    for neurons, activation in layers:\n",
    "        x = tf.keras.layers.Dense(math.ceil(neurons), activation=activation)(x)\n",
    "    encoder = x\n",
    "    for neurons, activations in rev_layers:\n",
    "        x = tf.keras.layers.Dense(math.ceil(neurons), activation=activation)(x)\n",
    "    x = tf.keras.layers.Dense(math.ceil(Ytrain.shape[1]), activation='sigmoid')(x)\n",
    "    decoder = x\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=x)\n",
    "    model.compile(optimizer=optimizer, loss=loss, \n",
    "                metrics=metrics)\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    model.fit(Xtrain, Ytrain, epochs=epochs, validation_data = (Xtest, Ytest),\n",
    "              batch_size=batch, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f4c57-a517-4702-8cf0-e24b149dceb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_excel(\"Συγκεντρωτικό Ελαιόλαδου.xlsx\", decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca7297-26d8-4bdb-bb13-01d367dec831",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Γενικά\n",
    "\n",
    "Τα δεδομένα μας απαρτίζονται από χημικές και φυσικοχημεικές παραμέτρους, παραμέτρους χαρακτηρισμού προέλευσης (ποικηλία, γεωγραφική προέλευση) και δεδομένα εδαφικών χαρακτηριστικών. Τα εδαφικά δεδομένα είναι λιγοστά ώστε πρακτικά δεν θα χρησιμοποιηθούν καθόλου. Η ποικηλία εμπεριέχει και μικτούς χαρακτηρισμούς (πχ 60% αδραμυτιανή, 40% κολοβή). Οι παράμετροι χαρακτηρισμού προέλευσης (και μεθόδου παραγωγής) είναι κατηγορικές μεταβλητές και αντιπροσωπεύονται με \"ενα σε Κ κωδικοποίηση\" *(one-to-k encoding)*.\n",
    "Οι χημικές παράμετροι διακρίνονται σε\n",
    "1. αντιοξειδωτικά\n",
    "2. χρωστικές\n",
    "3. dags\n",
    "4. στερόλες\n",
    "5. λιπαρά οξέα\n",
    "6. κήρους\n",
    "7. αιθυλεστέρες\n",
    "\n",
    "Οι φυσικοχημικές παράμετροι είναι:\n",
    "\n",
    "1. οξύτητα\n",
    "2. περοξείδια\n",
    "3. συντελεστής ($Κ232$)\n",
    "4. συντελεστής ($Κ270$)\n",
    "5. συντελεστής ($ΔK$)\n",
    "\n",
    "Οι οργανοληπτικές παράμετροι θα μοντελοποιηθούν χωριστά λόγο ιδιαιτεροτήτων τους και είναι:\n",
    "1. Διάμεση τιμή φρουτώδους ($M_f$)\n",
    "2. Διάμεση τιμή πικρού ($M_b$)\n",
    "3. Διάμεση τιμή πικάντικου ($M_p$)\n",
    "4. Διάμεση τιμή ελαττώματος ($M_d$) \n",
    "5. Παρατηρήσεις\n",
    "\n",
    "Οι παράμετροι προέλευσεις μπορούν να διακριθούν περαιτέρο σε παραμέτρους \"καλλιέργειας\" (*cultivar*) και παραμέτρους παραγωγής (ή ελαιοποίησης *oilification*). Οι παράμετροι καλλιέργειας είναι:\n",
    "\n",
    "1. Ποικηλία:\n",
    "     * Κολοβή\n",
    "     * Αδραμυτιανή\n",
    "     * Λέσσινο\n",
    "     * Λαδοελιά\n",
    "     * Αγρινιότικη\n",
    "     * Πατρινή\n",
    "     * Κρητική\n",
    "     * Κορωνέικη\n",
    "     * Αρμπεκίνα\n",
    "     * Χονδροελιά\n",
    "     * Θρούμπα\n",
    "     * Δαφνοελιά\n",
    "     * Μανάκι\n",
    "     * Άμφυσας\n",
    "     * Καλάμων\n",
    "     * Χιώτικη\n",
    "2. Γεωγραφική Προέλευση:\n",
    "    * Λέσβος\n",
    "    * Σάμος\n",
    "    * Χίος \n",
    "    * Ικαρία \n",
    "    * Φούρνοι\n",
    "3. Άρδευση (boolean)\n",
    "4. Λίπανση* (Κοπριά, Λίπανση, Ναι, Όχι) !!\n",
    "5. Ωρίμανση (Π, ΠΑ, ΠΙ, Μ) (k-enc)\n",
    "6. Οργανική Καλλιέργεια (boolean)\n",
    "7. Υψόμετρο (Ορεινό, Ημιορεινό, Πεδινό) (k-enc)\n",
    "\n",
    "Οι παράμετροι ελαιοποίησης είναι:\n",
    "\n",
    "1. Ημέρες έως ελαιοποίηση\n",
    "2. Μέθοδος συλλογής\n",
    "    * Τεμπ\n",
    "    * Μηχανικά (Μηχ. ελαι.)\n",
    "    * Ελαίοδ\n",
    "    * Δίχτυ\n",
    "    * Σακ. Δικτ\n",
    "    * Χτένι\n",
    "3. Τύπος Φυγοκέντρισης (boolean)\n",
    "4. Προσθήκη Νερού (boolean)\n",
    "5. Χρόνος Μάλαξης (int>0)\n",
    "6. Θερμοκρασία Μάλαξης (int>0)\n",
    "7. Θερμοκρασία Διαχωριστήρα (int>0)\n",
    "\n",
    "  Για ορισμένες ποικηλίες έχουμε ελάχιστες παρατηρήσεις (Πατρινή, Κρητική, Άμφυσας, Καλάμων) και δύσκολα μπορούμε να βγάλουμε συμπεράσματα. Η Λέσβος έχει διαχωριστεί σε υπό περιοχές - ζώνες. Θα μπορούσε να κατασκευαστεί μοντέλο γι αυτές ξεχωριστά (αφού οι υπόλοιπες περιοχές δεν διακρίνονται σε ζώνες). Για την λίπανση, έχουμε καταγραφές μορφής ΝΑΙ/ΟΧΙ στη μία παρτίδα και αναλυτικότερη (ΟΧΙ, ΚΟΠΡΙΑ, ΛΙΠΑΣΜΑ) στην άλλη. Συνεπώς αρχικά θα αναγάγουμε όλες τις καταγραφές σε  ΝΑΙ/ΟΧΙ χρησιμποιώντας και τα δύο σύνολα δειγμάτων και αργότερα μπορούμε να κατασκευάσουμε μοντέλο ιδιαίτερα για τη διάκριση Κοπριάς/Λιπάσματος. \n",
    "  Υπάρχουν αρκετές ελλειπής καταγραφές στις άνω κατηγορίες, συνεπώς αρχικά θα τις απορρίψουμε εντελώς και κατόπιν θα επιχειρήσουμε *imputation*. Οι περισσότερες κατηγορικές μεταβλητές-στόχοι προέλευσης μπορούν να αντιπροσωπευθούν με κ-κωδικοποίηση. Ιδιαίτερη περίπτωση αποτελεί η ποικηλία. Επειδή αυτή είναι σε πολλά δείγματα μεικτή, με γνωστές αναλογές, θα επιχειρήσουμε διαδικασία αντίστοιχη της κ-κωδικοποίησης, επιλύωντας πρόβλημα βελτιστοποίησης **υπό συνθήκες**. Θα επιχειρήσουμε να κατασκευάσουμε μοντέλο παλινδρόμησης ποσοστού κάθε ποικηλίας υπό συνθήκη αθροίσματος στη μονάδα. Για όλες τις υπόλοιπες παραμέτρους προέλευσης μπορούμε να χρησιμοποιήσουμε οποιοδήποτε μοντέλο classification, και επειδή η προέλευση έχει ιδιαιτερώτητες θα κατασκευάσουμε το αντίστοιχο μοντέλο χωριστά, τουλάχιστον στα αρχικά στάδια. Για τις μεθόδους συλλογής χρειαζομαστε διευκρινήσεις.\n",
    "\n",
    " Πιθανά μοντέλα ενδιαφέροντως\n",
    " * Γεωγραφικές Παράμετροι από χημικές\n",
    " * Οργανοληπτικές + Φυσικοχημικές από χημικές\n",
    " * Οργανοληπτικές + Φυσικοχημικές από προέλευση (path analysis?)\n",
    " * Σχέση παραμέτρων παραγωγής και προέλευσης σε ποιοτικές\n",
    " * SEM σε παραμέτρους ποιότητας"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22731eee-f794-413c-870a-bc9280206152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multindex = [\n",
    "            np.array(['id']*3 + ['origin']*54 + ['chemical']*44 +['phys']*10+['chemical']*37 + ['soil']*20),\n",
    "            np.array(['id']*3 + ['location']*2 + ['cultivar']*23+['cultivation']*9 +['production']*20+['antiox']*27+['pigm']*8+['dags']*9+['phys']*5+['org']*5+['f_acids']*15+['sterols']*20+['waxes', 'ethylesters']+['soil']*20),\n",
    "            np.array(df_raw.columns.to_list())]\n",
    "pdf = pd.DataFrame(data = df_raw.values, columns = multindex)\n",
    "pdf.replace(\n",
    "    {()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fb26f-94e9-4d27-8e31-af575b8ee06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Throw away problematic columns, too many missing values. Sum does not contain unique information\n",
    "coredf = pdf.drop(columns=['phph','pph','dtoc','syrig','obs', 'eryth', 'ouv', 'bcarot', 'bctoc', 'sum', 'kadr'],level=2)\n",
    "coredf = coredf.drop(columns=['soil'],level=0)\n",
    "coredf = coredf.drop(columns=['dags'],level=1)\n",
    "coredf = coredf.drop(columns=['rvar','org', 'alt', 'fert', 'mat', 'colmeth', 'watadd'], level=2)\n",
    "# Replace <0.01 entries with zeros\n",
    "coredf.replace('<0,01',0.00, inplace=True)\n",
    "coredf.loc[:,[('chemical', 'f_acids', '1trans'),('chemical', 'f_acids', '23trans')]] = coredf.loc[:,[('chemical', 'f_acids', '1trans'),('chemical', 'f_acids', '23trans')]].astype('float64')\n",
    "coredf.loc[:, 'origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46517a-2b62-4727-a32e-c149af5afe99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "altitude_mapping = {'Mountainous': 3, 'Planar': 2, 'Semimountainous': 1}\n",
    "location_mapping = {i:label  for i, label in enumerate(coredf.loc[:,('origin', 'location','loc')].unique(), 1)}\n",
    "_c1 = [\"colmeth_eld\", \"colmeth_eldi\",\"colmeth_mhx\", \"colmeth_temp\", \"colmeth_dixt\", \"colmeth_xte\"]\n",
    "bin_map = {0.0:'No', 1.0: 'Yes', 1:'Yes', 0:'No'}\n",
    "_c2 = {('origin', 'production', v):bin_map for v in _c1}\n",
    "_c3 = [\"mat_null\", \"mat_pa\", \"mat_pi\", \"mat_p\", \"mat_m\"]\n",
    "_c4 = {('origin', 'production', v):bin_map for v in _c3}\n",
    "core_mappings = {\n",
    "        ('origin', 'cultivation', 'org_b'): {0:'Conventional', 1: 'Organic', 0.0:'Conventional', 1.0:'Organic'},\n",
    "        ('origin', 'cultivation', 'fert_b'): {0: 'No', 1: 'Fertilized'},\n",
    "        ('origin', 'cultivation', 'wat'): {1: 'Yes', 0: 'No'},\n",
    "        ('origin', 'cultivation', 'altitude'): invert_dict(altitude_mapping),\n",
    "        ('origin', 'location', 'loc') : location_mapping,\n",
    "        (\"origin\", \"production\", \"watadd_b\"):{0.0:\"No\", 1.0:\"Yes\"},\n",
    "        ('origin', 'production', 'centr'):{3.0:\"3PH\", 2.0:\"2PH\"}\n",
    "    }\n",
    "r_core_mappings = {k: invert_dict(v) for k, v in core_mappings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8957e-1fd3-470d-ba74-945fe0926b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = [('origin', 'cultivation', 'alt_ped'), ('origin', 'cultivation', 'alt_or'), ('origin', 'cultivation', 'alt_sem')]\n",
    "cdf = undummify(coredf, cols =c,rmap={('origin', 'cultivation', 'altitude'):invert_dict(altitude_mapping)}, ncol_name=('origin', 'cultivation', 'altitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca857d-4e5b-4c53-ac30-ff022fc6467b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For some reason the replace doesn't work properly on 'watadd_b' when included with all the others, but does work when handdled sepperatly.\n",
    "cdf = cdf.replace(core_mappings|_c2|_c4).replace({(\"origin\", \"production\", \"watadd_b\"):{1.0:\"Yes\", 0.0:\"No\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c24acc-4457-46fd-a09b-ae2a71a5bb07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_c = [(\"origin\",\"location\", \"zn\"), (\"origin\",\"location\", \"loc\")]+list(_c2.keys())+list(_c4.keys())+[\n",
    "    (\"origin\", \"production\", \"centr\"), (\"origin\", \"production\", \"watadd_b\"), (\"origin\", \"cultivation\", \"altitude\"),(\"origin\", \"cultivation\", \"org_b\")]\n",
    "cdf.loc[:,('origin', 'location', 'zn')] = cdf.loc[:,('origin', 'location', 'zn')].astype(pd.Int64Dtype())\n",
    "cdf.loc[:,_c]=cdf.loc[:,_c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf86761-b6f9-442b-b167-5ee0510b2314",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Διερευνιτική Ανάλυση Δεδομένων"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593f748-de4a-49c1-a388-4d6bb4b961f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Περιγραφική Στατιστική"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52252733-e6ab-4b4b-a31d-e1e1cfabf15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf[cdf.loc[:, (\"origin\",\"cultivar\")].isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68159569-dd76-4bc4-b4bd-e0a5a9ad160d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_display_once(coredf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237eae0-b865-42e7-b17d-35e851968a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_display_once(cdf.loc[:,_c ].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0f8ff-f4f3-49c4-a97c-55c33089f22a",
   "metadata": {},
   "source": [
    "### Συσχετίσεις"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce3380-8f6a-4bac-b5bc-01dd3eb92bc8",
   "metadata": {},
   "source": [
    "#### Γενικές"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df01461-87db-4b3c-9319-7bc4b93b8563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cdf.corr().applymap(np.abs), cmap='viridis', xticklabels=True, yticklabels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48072d1-20aa-4723-a352-d9be99dd25c7",
   "metadata": {},
   "source": [
    "#### Χημικές Μεταβλητές"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f47dc3-3ae4-4c59-aa9d-dc4b531f8881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cdf.loc[:,'chemical'].corr().applymap(np.abs), cmap='viridis', xticklabels=True, yticklabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabb882-cbb3-41d4-aa04-1cb34dd0307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.loc[:,[('origin','location','zn'), ('origin','location','loc') ]+ list((\"origin\",\"cultivation\", e) for e in cdf.loc[:,('origin', 'cultivation')].columns )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230309c-2109-4cc2-8e72-cc66fd420d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data selection\n",
    "origin_targets = cdf.loc[:,(\"origin\", ['cultivation','location'])  ].drop(('origin', 'location', 'zn'), axis=1)\n",
    "m1 = origin_targets.isna().any(axis=1)\n",
    "chem_indicators =cdf.loc[:,(['chemical'])]\n",
    "m2 = chem_indicators.isna().any(axis=1)\n",
    "m = pd.concat([m1, m2],axis=1).any(axis=1)\n",
    "chem_indicators, origin_targets = chem_indicators.loc[~m,:], origin_targets.loc[~m,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2e4d9-00d4-4d4d-9ac9-da1f4e53d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = rowwise_value_counts(origin_targets).astype(int)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600935e1-b2cc-41b8-9a08-10ddac587b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# origin_targets=origin_targets.astype('int32')\n",
    "# origin_targets.loc[:,('cultivation', 'alt_ped')]=  origin_targets.loc[:,('cultivation', 'alt_ped')]*2\n",
    "# origin_targets.loc[:,('cultivation', 'alt_or')]=  (origin_targets.loc[:,('cultivation', 'alt_or')]*3)\n",
    "# origin_targets.loc[:,('cultivation', 'alt')] = origin_targets.iloc[:,1:4].values.argmax(axis=1)\n",
    "# origin_targets.drop(columns=[('cultivation' ,'alt_sem'), ('cultivation' ,'alt_ped'), ('cultivation' ,'alt_or')],inplace=True)\n",
    "# origin_targets.loc[:,('cultivation', 'alt')] =origin_targets.loc[:,('cultivation', 'alt')]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a496a7-d221-4cfa-ae26-91625b82e58c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mappings = {\n",
    "#         ('cultivation', 'org_b'): {0:'Conventional', 1: 'Organic'},\n",
    "#         ('cultivation', 'fert_b'): {0: 'No', 1: 'Fertilized'},\n",
    "#         ('cultivation', 'wat'): {1: 'Yes', 0: 'No'},\n",
    "#         ('cultivation', 'alt'): invert_dict(altitude_mapping),\n",
    "#         ('location', 'loc') : invert_dict(location_mapping)\n",
    "#     }\n",
    "# rmappings = {k: invert_dict(v) for k, v in mappings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a87c4-09db-44b9-a4cf-ff25510ee275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# origin_targets = origin_targets.replace(mappings)\n",
    "# origin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ba66c-e6b7-4f02-99f6-7bdb155b19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = categorical_scatter(chem_indicators, origin_targets, cols=4, categorical='hue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3097b3e5-c91d-4abc-8e24-eec73ca0dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = next(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9dbbbe-77a8-4fe6-94d8-219120059769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from pca import pca\n",
    "\n",
    "# Initialize pca with default parameters\n",
    "model = pca(normalize=True)\n",
    "\n",
    "# Fit transform\n",
    "\n",
    "results = model.fit_transform(chem_indicators.values, col_labels = tidy_multiindex(chem_indicators).columns.values, row_labels=tidy_multiindex(origin_targets).replace(r_core_mappings).iloc[:,-2].values)\n",
    "\n",
    "# Plot the explained variance\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dce184-af08-45b6-8249-71169b0ce546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.biplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e5916-822d-46ba-8dcc-0feac08a5ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(model.results['loadings'].applymap(np.abs).T,cmap='viridis', xticklabels=True, yticklabels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38548d8-9bec-4508-8a65-31a5dc9f29ab",
   "metadata": {},
   "source": [
    "## Μοντελοποίηση"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c4654-5d14-4505-8cfb-aac60b4d5137",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Χαρακτηριστικά Γεωγραφικής Προέλευσης"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003047e-bc9b-43ac-91bb-7ecc9163b97b",
   "metadata": {},
   "source": [
    "#### Τοποθεσία"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac87a1-63c6-4d20-9a64-540a12abec33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rand_forest():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, multilabel_confusion_matrix, confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from itertools import product\n",
    "    model = RandomForestClassifier(criterion='entropy', max_features=None, n_estimators=100)\n",
    "    X = pd.DataFrame(data=StandardScaler().fit_transform(chem_indicators), columns = chem_indicators.columns, index=chem_indicators.index)\n",
    "    Y = origin_targets.replace(core_mappings)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
    "\n",
    "    c_matrix= {}\n",
    "\n",
    "    for col in Y_train.columns:\n",
    "        model.fit(X_train, Y_train.loc[:,col])\n",
    "        levels = Y_train.loc[:, col].unique()\n",
    "        Y_hat = pd.DataFrame(data=model.predict(X_test))\n",
    "        c_matrix[col] = pd.DataFrame(data=confusion_matrix(Y_test.loc[:,col], Y_hat, labels=[v for k, v in core_mappings[col].items()]\n",
    "                                                   ),\n",
    "                                     index = [('True', level) for level in levels], columns = [('Predicted', level) for level in levels] )\n",
    "    fig, axs = plt.subplots(Y.shape[1]%3, 3)\n",
    "    fig.suptitle(\"Confusion matrices for Random Forest Classifiers\")\n",
    "    for (i,j), (var, matrix) in zip(product(range(Y.shape[1]%3), range(3)), c_matrix.items()):\n",
    "        sns.heatmap(c_matrix[var], annot=True, cmap='viridis', ax=axs[i,j])\n",
    "        axs[i,j].set_title(var, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0a5f1-4cf2-4b0a-a8be-eeb380f54e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, multilabel_confusion_matrix, confusion_matrix\n",
    "# from itertools import product\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# validation_iters = 10\n",
    "# # VERY messy. Look into xarray\n",
    "# t1 = {}\n",
    "# acc = {}\n",
    "# s = True\n",
    "# for i in range(validation_iters):\n",
    "#     model = RandomForestClassifier(criterion='entropy', max_features=None, n_estimators=100)\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "#     for col in Y.columns:\n",
    "#         model.fit(X_train, Y_train.loc[:,col])\n",
    "#         levels = Y_train.loc[:, col].unique()\n",
    "#         Y_hat = model.predict(X_test)\n",
    "#         c = confusion_matrix(Y_test.loc[:,col], Y_hat, labels=[v for k, v in mappings[col].items()], normalize='true')\n",
    "#         t1[col] = c\n",
    "#     if s:\n",
    "#         s=not s\n",
    "#         t2 = t1\n",
    "#     else:\n",
    "#         for k, v in t1.items():\n",
    "#             t2[k] = np.dstack(tuple([v, t1[k]]))\n",
    "# for k, v in t2.items():\n",
    "#             levels = Y.loc[:,k].unique()\n",
    "#             data = np.average(v, axis=2)\n",
    "#             t2[k] = pd.DataFrame(data = data, index = [('True', level) for level in levels], columns = [('Predicted', level) for level in levels])\n",
    "            \n",
    "# fig, axs = plt.subplots(Y.shape[1]%3, 3)\n",
    "# fig.suptitle(\"Confusion matrices for Random Forest Classifiers\")\n",
    "# for (i,j), (var, matrix) in zip(product(range(Y.shape[1]%3), range(3)), t2.items()):\n",
    "#     sns.heatmap(t2[var], annot=True, cmap='viridis', ax=axs[i,j])\n",
    "#     axs[i,j].set_title(var, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b8fb8-3e2b-41af-9283-632c49bf885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_reg():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.inspection import DecisionBoundaryDisplay\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import r2_score as r2\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    from itertools import product\n",
    "    from math import ceil\n",
    "    import pprint\n",
    "\n",
    "    X = chem_indicators\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(chem_indicators, origin_targets, test_size=0.2, random_state=44)\n",
    "\n",
    "    X_train, X_test = std_scale(X_train), std_scale(X_test)\n",
    "    targets = Y.columns\n",
    "    models = {}\n",
    "\n",
    "    for target in targets:\n",
    "        factors =  origin_targets.loc[:,target].unique()\n",
    "        y_train = Y_train.loc[:,target]\n",
    "        y_test =  Y_test.loc[:, target]\n",
    "        logitmodel = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "        logitmodel.fit(X_train, y_train)\n",
    "        y_pred = logitmodel.predict(X_test)\n",
    "        model = dict(\n",
    "            model = logitmodel,\n",
    "            error_matrix = pd.DataFrame(data=confusion_matrix(y_test, y_pred, labels =factors),\n",
    "                                        columns = [('Predicted', e) for e in factors],\n",
    "                                        index = [('True', e) for e in factors]\n",
    "            ),\n",
    "            accuracy = accuracy_score(y_test, y_pred),\n",
    "            score = logitmodel.score(X_train, y_train),\n",
    "            decision_function = logitmodel.decision_function(X_train))\n",
    "        models[target] = model\n",
    "\n",
    "    rows = ceil(len(models)/3)\n",
    "\n",
    "    fig, axs = plt.subplots(ncols = 3, nrows = rows )\n",
    "    fig.suptitle(\"Confusion matrices for Logistic Regression Classifier on Location target variables\")\n",
    "    for (i, j), (target, model) in zip(product(range(3), range(rows) ), models.items()):\n",
    "        sns.heatmap(model['error_matrix'], annot=True, cmap='viridis', ax = axs[j, i])\n",
    "        axs[j,i].set_title(target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05837d-5e42-4bef-8de2-5f0aa2373093",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowwise_value_counts(origin_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d9177-e7c6-4abe-b05c-794c43e1a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance():\n",
    "    from imblearn.over_sampling import ADASYN, SMOTE\n",
    "\n",
    "    X = chem_indicators\n",
    "    Y = origin_targets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(chem_indicators, origin_targets, test_size=0.2, random_state=44)\n",
    "    X_train, X_test = std_scale(X_train), std_scale(X_test)\n",
    "    targets = Y.columns\n",
    "    models = {}\n",
    "\n",
    "    for target in targets:\n",
    "        factors =  origin_targets.loc[:,target].unique()\n",
    "        y_train = Y_train.loc[:,target]\n",
    "        y_test =  Y_test.loc[:, target]\n",
    "        logitmodel = LogisticRegression(max_iter=10000)\n",
    "        adasyn = ADASYN(random_state=44, sampling_strategy='minority', n_neighbors=2)\n",
    "        smote = SMOTE(k_neighbors=2)\n",
    "        logitmodel.fit(*smote.fit_resample(X_train, y_train))\n",
    "        y_pred = logitmodel.predict(X_test)\n",
    "        model = dict(\n",
    "            model = logitmodel,\n",
    "            error_matrix = pd.DataFrame(data=confusion_matrix(y_test, y_pred, labels =factors),\n",
    "                                        columns = [('Predicted', e) for e in factors],\n",
    "                                        index = [('True', e) for e in factors]\n",
    "            ),\n",
    "            accuracy = accuracy_score(y_test, y_pred),\n",
    "            score = logitmodel.score(X_train, y_train),\n",
    "            decision_function = logitmodel.decision_function(X_train))\n",
    "        models[target] = model\n",
    "\n",
    "    rows = ceil(len(models)/3)\n",
    "\n",
    "    fig, axs = plt.subplots(ncols = 3, nrows = rows )\n",
    "    fig.suptitle(\"Confusion matrices for Logistic Regression Classifier on Location target variables, with ADASYN\")\n",
    "    for (i, j), (target, model) in zip(product(range(3), range(rows) ), models.items()):\n",
    "        sns.heatmap(model['error_matrix'], annot=True, cmap='viridis', ax = axs[j, i])\n",
    "        axs[j,i].set_title(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbdb69f-f3cd-410d-ade9-4ce1cdac596a",
   "metadata": {},
   "source": [
    "Παρατηρούμε γενικά ικανοποιητική απόδοση τις εκτιμήτριας ως προς τη γεωγραφική προέλευση και πολύ κακή απόδωση τις υπόλοιπες περιπτώσεις. Λαμβάνουμε υπ' όψιν οτι τα"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c07e2-b2cd-4490-86b6-cab36342d642",
   "metadata": {},
   "source": [
    "##### ANN με GEKKO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36acbe41-d2ed-4ba2-be6f-59dd13ca006c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gekko_ANN(X:pd.DataFrame, Y:pd.DataFrame, layers:list[tuple[str,typing.Optional[int]]]=[('relu', None)],\n",
    "             exit_nodes:typing.Optional[int]=None, exit_activation:str='linear'):\n",
    "    from gekko import brain, GEKKO\n",
    "    model = GEKKO(remote=False)\n",
    "    model.options.MAX_MEMORY = 7    \n",
    "    br = brain.Brain(m=model)\n",
    "    N, M = Y.shape[1], X.shape[1]\n",
    "    dstring = lambda e: e if e is not None else 'relu'\n",
    "    istring = lambda e: e if e is not None else M\n",
    "    processed_layers = [(dstring(e1), istring(e2)) for (e1, e2) in layers]\n",
    "    ext_sz = exit_nodes if exit_nodes is not None else N\n",
    "    br.input_layer(M)\n",
    "    for activation, nodes in processed_layers:\n",
    "        if activation == 'relu':\n",
    "            br.layer(relu=nodes)\n",
    "        elif activation == 'gaussian':\n",
    "            br.layer(gaussian=nodes)\n",
    "        elif activation == 'tanh':\n",
    "            br.layer(sigmoid = nodes)\n",
    "        elif activation == 'bent':\n",
    "            br.layer(bent=nodes)\n",
    "        elif activation == 'leaky':\n",
    "            br.layer(leaky=nodes)\n",
    "    br.output_layer(ext_sz, activation=exit_activation)\n",
    "    try:\n",
    "        br.learn(X.values.T, Y.values.T)\n",
    "    except FileNotFoundError:\n",
    "        raise RuntimeError('Encountered infinities. Network failed to converge')\n",
    "    return br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf7332-47e6-4df6-a2cc-8bf710f1d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data = sklearn.decomposition.PCA(n_components = .95).fit_transform(tidy_multiindex(chem_indicators) ))\n",
    "Y = tidy_multiindex(origin_targets.replace(r_core_mappings))\n",
    "\n",
    "# def deploy_daemon(func, *args, **kwargs):\n",
    "#     f = functools.partial(gekko_ANN,X, pd.get_dummies(Y), layers = [('relu', 44) for _ in range(4)] )\n",
    "#     multiprocessing.Process(target = )\n",
    "\n",
    "# f = functools.partial(gekko_ANN,X, pd.get_dummies(Y), layers = [('relu', 44) for _ in range(4)] )\n",
    "# p = multiprocessing.Process(target = f)\n",
    "# multiprocessing.Process(target= gekko_ANN(X, pd.get_dummies(Y), layers = [('relu', 44) for _ in range(4)])).start()\n",
    "\n",
    "# ann = gekko_ANN(X, pd.get_dummies(Y), layers = [('relu', 44) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6fdc0-24e5-4e51-b472-0766673e187a",
   "metadata": {},
   "source": [
    "#### ANN με Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c5a7a-815d-4712-a2da-cdbf187f41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder for feature selection\n",
    "\n",
    "# Data prep\n",
    "x = tidy_multiindex(chem_indicators)\n",
    "X = pd.DataFrame(data=sklearn.preprocessing.MinMaxScaler().fit_transform(x),\n",
    "                columns=x.columns, index=x.index)\n",
    "# Y = pd.get_dummies(tidy_multiindex(Y))\n",
    "Y = tidy_multiindex(origin_targets.replace(r_core_mappings))\n",
    "Xtrain, Xtest, Ytrain, Ytest =sklearn.model_selection.train_test_split(X, Y, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bb8ec-ae8e-4142-8458-d6673ef078ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import copy\n",
    "# rlayers = list((math.ceil(e), 'tanh') for e in range(X.shape[1], 2, -10) )\n",
    "# llayers = copy(rlayers)\n",
    "# llayers.reverse()\n",
    "# layers = rlayers+llayers\n",
    "# layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980335d4-9203-4ed2-b34d-f442d2f49779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_autoencoder(Xtrain, Xtrain, Xtrain, Xtrain, loss='mse', layers=rlayers, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20e93e-1d40-4166-ab70-9a94f1e05102",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [(math.ceil(45), 'sigmoid') for _ in range(1,6)]\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9977d6-f447-4070-99ba-2cdd574a84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(X, Y):\n",
    "    from imblearn.over_sampling import ADASYN\n",
    "    ada = ADASYN()\n",
    "    Xr, Yr = ADASYN().fit_sample(chem_indicators, origin_targets)\n",
    "    X = pd.DataFrame(data = sklearn.decomposition.PCA(n_components = .95).fit_transform(tidy_multiindex(Xr) ))\n",
    "    Y = tidy_multiindex(Yr.replace(r_core_mappings))\n",
    "    return Xr, Yr\n",
    "x,y=oversample(chem_indicators, origin_targets)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56d15a-31d5-4a23-b03e-7cfd8511463a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "m = tf_model(Xtrain, Xtest, Ytrain, Ytest, activation = 'relu', \n",
    "        loss='categorical_crossentropy', metrics = ['accuracy'],\n",
    "        epochs=4000, layers=layers , callbacks=[tensorboard_callback],\n",
    "        verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657ac43-0624-4d75-9edf-a9492c8ec0cb",
   "metadata": {},
   "source": [
    "### Ποικηλίες"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718dd3a8-054d-4d1a-8c92-f551cbec7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf[cdf.loc[:,('origin', 'cultivar')].isna().any(axis=1)].loc[:, ('origin', 'cultivar')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9538ee6-d28c-43ad-8b66-30f0091efa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pymc.Model() as var_model:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff7e56-abb9-469e-8ce9-69a660d91721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
