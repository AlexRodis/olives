{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66dde2a-d3f0-49a6-95b7-1dbaa9d68a14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 11:50:27.008595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2022-12-10 11:50:27.008994: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2022-12-10 11:50:27.009006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/media/alexander-fyrogenis/Elements/Διδακτορικό/Olive Oil/notebooks/evoos_env/lib/python3.10/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/media/alexander-fyrogenis/Elements/Διδακτορικό/Olive Oil/notebooks/evoos_env/lib/python3.10/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/media/alexander-fyrogenis/Elements/Διδακτορικό/Olive Oil/notebooks/evoos_env/lib/python3.10/site-packages/pymc/sampling/jax.py:37: UserWarning: This module is experimental.\n",
      "  warnings.warn(\"This module is experimental.\")\n",
      "/media/alexander-fyrogenis/Elements/Διδακτορικό/Olive Oil/notebooks/evoos_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Βιβλιοθήκες\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gekko as gk\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import typing\n",
    "import itertools\n",
    "import math\n",
    "import functools\n",
    "from pca import pca\n",
    "from IPython.display import display, HTML\n",
    "import prince\n",
    "import networkx\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import multiprocessing\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "import arviz as az\n",
    "import aesara\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import aesara.tensor as at\n",
    "import scipy\n",
    "import pymc as pm, pymc\n",
    "import gpflow\n",
    "from pymc.sampling.jax import sample_numpyro_nuts\n",
    "import numpyro\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5060b75a-dfce-4da4-a7d8-d5ae35922d01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Γενικές επιλογές εμφάνισης\n",
    "pd.set_option('display.max_rows',10)\n",
    "pd.set_option('display.max_columns',30)\n",
    "\n",
    "numpyro.set_platform(\"gpu\")\n",
    "\n",
    "figsize=(13,6)\n",
    "fontsize=10\n",
    "sns.set(rc={'figure.figsize':(15,10),\n",
    "           'font.size':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023dc91d-d210-4c2c-8f09-b2eb248cbf5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scale = lambda df: pd.DataFrame(data = StandardScaler().fit_transform(df), columns = df.columns, index=df.index)\n",
    "\n",
    "def render_df(df:pd.DataFrame):\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(df.to_html()))\n",
    "    return\n",
    "\n",
    "def full_display(r=None, c=None):\n",
    "    pd.set_option('display.max_rows', r)\n",
    "    pd.set_option('display.max_columns',c)\n",
    "    return\n",
    "\n",
    "def reset_display():\n",
    "    pd.set_option('display.max_rows',10)\n",
    "    pd.set_option('display.max_columns',30)\n",
    "\n",
    "def full_display_once(df:pd.DataFrame, r=None, c=None):\n",
    "    full_display(r=r, c=c)\n",
    "    render_df(df)\n",
    "    reset_display()\n",
    "\n",
    "def rowwise_value_counts(df:pd.DataFrame):\n",
    "    vcounts_df = pd.DataFrame(data = df.apply(lambda x: x.value_counts()).T.stack()).astype(int).T\n",
    "    vcounts_df.index = ['']\n",
    "    return vcounts_df\n",
    "\n",
    "invert_dict = lambda e: {v:k for k, v in e.items()}\n",
    "\n",
    "def categorical_scatter(X:pd.DataFrame, Y:typing.Optional[pd.DataFrame],cols:int=3,max_rows:int = 3,\n",
    "                             figsize:tuple[int, int] = (30, 15), xaxis_label_size: int=12,\n",
    "                             yaxis_label_size:int=12, categorical:typing.Optional[str]='hue')->typing.Optional[plt.figure]:\n",
    "    '''\n",
    "        Generate pair-wise scatter plots for a given DataFrame, with optional support for large Datasets\n",
    "        and categorical variables. Yields a figure with `max_rows x cols` scatterplots per call. When\n",
    "        `categorical=None` only generates pair-wise scatterplots for the columns of X, else generates all\n",
    "        combinations of X-column pair and Y columns. When `categorical='hue'` values of the categorical Y\n",
    "        variable will be depicted color-coded and when `categorical=size` different factors of the Y variable\n",
    "        will have different sizes instead.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "            - X:pandas.DataFrame := The data to depict. When Y is also specified, X is assumed to be the DataFrame\n",
    "            of indicator variables\n",
    "            \n",
    "            - Y:Optional[pandas.DataFrame] := Optional Dataframe of categorical variables to depict. Ignored if \n",
    "            `category` is `None`.\n",
    "            \n",
    "            - idx_lvl:int=1 := For multilevel indexed dataframes the lowest level to squash the index on. Must be non-negative\n",
    "            \n",
    "            - cols:int=3 := Number of columns for the resulting facet grid plot. Must be non-negative. Defaults to 3\n",
    "            \n",
    "            - max_rows:int=3 := Maximum number of rows per batch the generator yields. Defaults to 3 and must be non-negative\n",
    "            \n",
    "            - figsize:tuple[int, int] := A `height x width` tuple for the generated plots. Defaults to `(30, 15)`\n",
    "            \n",
    "            - xaxis_label_size:int=12 := The size of the x-axis titles for each subplot. Must be non-negative and defaults to 12\n",
    "            \n",
    "            - yaxis_label_size:int=12 := The size of the y-axis titles for each subplot. Must be non-negative and defaults to 12\n",
    "            \n",
    "            - categorical:Optional[str] := Set the display of the categorical variable. (1) `None` ignores `Y` and only displays X\n",
    "            pair-wise scatterplots, (2) 'hue' displays the categorical variable of `Y` as color and (3) 'size' displays\n",
    "            the categorical variable with differently sized points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        \n",
    "            - fig:matplotlib.pyplot.figure := The figure object, a FacetPlot of scatterplots\n",
    "            \n",
    "       \n",
    "       Raises:\n",
    "        ------\n",
    "        \n",
    "            - WIP\n",
    "            \n",
    "            - ValueError \n",
    "    '''\n",
    "    XY = pd.concat([Y, X], axis=1)\n",
    "    idx_lvl= XY.columns.nlevels-1\n",
    "    skip_multiindex = lambda df,i , idx_level=idx_lvl: df.columns[i][idx_level] if idx_level else df.columns[i]\n",
    "    if idx_lvl:\n",
    "        XY.columns = XY.columns.get_level_values(idx_lvl)\n",
    "    x_combs = math.comb(X.shape[1], 2)\n",
    "    MAX_ROWS = max_rows\n",
    "    \n",
    "    if categorical is not None:\n",
    "        plots = x_combs*Y.shape[1]\n",
    "    else:\n",
    "        plots = x_combs\n",
    "    ncols = cols\n",
    "    size_scaling_factor = None\n",
    "    total_rows = math.ceil(plots/ncols)\n",
    "    total_figures = math.ceil(total_rows/MAX_ROWS)\n",
    "    X_pairs = itertools.combinations(range(X.shape[1]), 2)\n",
    "    subplots = itertools.product(X_pairs,range(Y.shape[1]) ) if categorical is not None else X_pairs\n",
    "    exhaustion_sentinel = False\n",
    "    while True:\n",
    "        if exhaustion_sentinel: break\n",
    "        fig, axs = plt.subplots(nrows=MAX_ROWS, ncols=ncols, figsize=figsize)\n",
    "        ax_indices = itertools.product(range(MAX_ROWS), range(ncols), repeat=1)\n",
    "        plotslice = itertools.islice(subplots, MAX_ROWS*ncols)\n",
    "        fig_generator = itertools.zip_longest( ax_indices, plotslice, fillvalue = ((None, None), None) )\n",
    "        for (axi, axj), e in fig_generator:\n",
    "            e = tuple(flatten(e))\n",
    "            if e[0] is not None:\n",
    "                if categorical == 'size':\n",
    "                    sns.scatterplot(x=skip_multiindex(X, e[0]), y=skip_multiindex(X, e[1]), data=XY,\n",
    "                       size=skip_multiindex(Y, e[2]), ax=axs[axi, axj], legend=True)\n",
    "                elif categorical == 'hue':\n",
    "                    sns.scatterplot(x=skip_multiindex(X, e[0]), y=skip_multiindex(X, e[1]), data=XY,\n",
    "                       hue=skip_multiindex(Y, e[2]), ax=axs[axi, axj], legend=True)\n",
    "                elif categorical is None:\n",
    "                    sns.scatterplot(x=skip_multiindex(X, e[0]), y=skip_multiindex(X, e[1]), data=XY,\n",
    "                       ax=axs[axi, axj], legend=True)\n",
    "                axs[axi, axj].set_xlabel(skip_multiindex(X, e[0]))\n",
    "                axs[axi, axj].set_ylabel(skip_multiindex(X, e[1]))\n",
    "                axs[axi, axj].xaxis.label.set_size(xaxis_label_size)\n",
    "                axs[axi, axj].yaxis.label.set_size(yaxis_label_size)\n",
    "                \n",
    "            else:\n",
    "                axs[axi, axj].axis('off')\n",
    "                exhaustion_sentinel = True\n",
    "        yield fig \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def flatten(xs):\n",
    "    from collections.abc import Iterable\n",
    "    for x in xs:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "def tidy_multiindex(df:pd.DataFrame, sep:str=\".\"):\n",
    "    '''\n",
    "        Compress a hierarchically indexed dataframe to standardized tidy\n",
    "        format. A unique sepperator `sep` is used to allow reversal. All\n",
    "        levels of the index are appended together with a delimeter to allow\n",
    "        reversals.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        \n",
    "            - df:pandas.DataFrame := A `pandas.DataFrame` hierarchically indexed\n",
    "            \n",
    "            - sep:str='_._' := A delimenter delineating the different levels\n",
    "            of the index. Ensure it is not present in any column name to avoid\n",
    "            a malformed index\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        \n",
    "            - ndf:pandas.DataFrame := The DataFrame with a single-level index\n",
    "    '''\n",
    "    tidy_cols = (functools.reduce(lambda e1,e2: str(e1)+sep+str(e2), col ) for col in df.columns)\n",
    "    ndf = df.copy(deep=True)\n",
    "    ndf.columns = tidy_cols\n",
    "    return ndf\n",
    "\n",
    "def reverse_tidy_multiindex(df:pd.DataFrame, sep=\".\"):\n",
    "    '''\n",
    "        Reverses the tidying to a hierachical format. Different\n",
    "        levels of the index are identified based on \"sep\"\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "            - df:pandas.DataFrame := The dataframe to process\n",
    "            \n",
    "            - sep:str='_._' := The string delimeter, sepperating\n",
    "            values for different levels of the index\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        \n",
    "            - ndf:pandas.DataFrame := The dataframe with hierarchical index\n",
    "    '''\n",
    "    h_cols = (tuple(col.split(sep)) for col in df.columns)\n",
    "    ndf = df.copy(deep=True)\n",
    "    ndf.columns = pd.MultiIndex.from_tuples(h_cols)\n",
    "    return ndf\n",
    "\n",
    "def undummify(df:pd.DataFrame,cols:list[str, tuple[str]],ncol_name:typing.Union[str,tuple[str]],\n",
    "              sep:typing.Optional[str]=None,\n",
    "              rmap:typing.Optional[dict[int, typing.Union[str, tuple[str]]]]=None\n",
    "             )->pd.DataFrame:\n",
    "    '''\n",
    "        Reverses hot-encoded variables in the DataFrame. A series of hot-encoded\n",
    "        variable levels $(i_1, i2, \\dots, i_k)$ is mapped to a single new column\n",
    "        $(k)$, whose name is specified by `ncol_name`, in the new dataframe. Pre\n",
    "        vious level columns are dropped.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        \n",
    "            - df:pandas.DataFrame := The DataFrame to operate upon\n",
    "            \n",
    "            - cols:list[str, tuple[str]] := A list of columns, representing the\n",
    "            levels of a categorical variable\n",
    "            \n",
    "            - sep:Optional[str] := sepperator for variable level. Currently ignored\n",
    "            \n",
    "            - ncol_name:Union[str, tuple[str]] := Name of the new categorical column\n",
    "            \n",
    "            - remap:Optional[dict[int, Union[str, tuple[str]]]] := A dictionary mapping\n",
    "            of categorical levels to values. Keys are the assumed to be levels, values\n",
    "            are assumed to be values (i.e. strings). When provided, the previous levels\n",
    "            will be replaced by the specified mappings in the new DataFrame\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        \n",
    "            - ndf:pandas.DataFrame := The processed dataframe\n",
    "     '''\n",
    "    _df = coredf.loc[:, cols]\n",
    "    for i, col in enumerate(cols, 1):\n",
    "        _df.loc[:, col] = i*_df.loc[:, col]\n",
    "    ndf = df.copy(deep=True)\n",
    "    ndf.drop(cols, axis=1, inplace=True)\n",
    "    ndf[ncol_name] = _df.max(axis=1)\n",
    "    c1 = df.columns.tolist()\n",
    "    i = c1.index(cols[0])\n",
    "    swp = ndf.columns.tolist()[:i-1]+[ndf.columns.tolist()[-1]]+ndf.columns.tolist()[i:-1]\n",
    "    ndf = ndf.loc[:, swp]\n",
    "    if rmap is not None:\n",
    "        ndf = ndf.replace(rmap)\n",
    "    return ndf\n",
    "list_difference = lambda l1, l2: [e for e  in l1 if e not in set(l2)]\n",
    "\n",
    "def corrspace_graph(df:pd.DataFrame):\n",
    "    '''\n",
    "        Generate a correlation graph representation of `df` datasets.\n",
    "        Every node in the resulting graph is a variable and every \n",
    "        vertex between two nodes represents the correlation between\n",
    "        the two variables, the correlation being the verted weight\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        \n",
    "            - df:pandas.DataFrame := A dataset to depict\n",
    "            \n",
    "        Returns:\n",
    "            - WIP\n",
    "    '''\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "class DictTable(dict):\n",
    "    '''\n",
    "        Jupyter utility class that overrides the dicts' defaults\n",
    "        __repr__ rendering the input dictionary to an HTML table\n",
    "        for convenient jupyter rendering\n",
    "    '''\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for key, value in self.items():\n",
    "            html.append(\"<tr>\")\n",
    "            html.append(\"<td>{0}</td>\".format(key))\n",
    "            html.append(\"<td>{0}</td>\".format(value))\n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "    \n",
    "def boxplots(df:pd.DataFrame, max_features:int=20, title:str=\"Boxplot\",\n",
    "             **kwargs)->typing.Generator[plt.Figure, None, None]:\n",
    "    '''\n",
    "        Wrapper around `pandas.boxplot` for datasets with a large number\n",
    "        of features. Returns a generator of boxplots of subsets of the total\n",
    "        features.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "            - df:pandas.DataFrame := The DataFrame to plot\n",
    "            \n",
    "            - max_features:int>0 := The maximum number of features to plot\n",
    "            in each generated boxplot\n",
    "            \n",
    "            - title:str='Boxplot' := The title of the plot. A subplot index\n",
    "            of the form **(X/Y)**, tracking the current plot will be\n",
    "            appended prior to rendering\n",
    "            \n",
    "            - **kwargs:dict[str:Any] := Keyword arguments to be be forwarded\n",
    "            to pandas.DataFrame.boxplot\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        \n",
    "            - Generator[plt.Figure, None, None] := A generator yielding boxplots\n",
    "            of feature subsets\n",
    "    '''\n",
    "    nplots = math.ceil(df.shape[1]/max_features)\n",
    "    for i in range(nplots):\n",
    "        if (i+1)*max_features<df.shape[1]:\n",
    "            ndf = df.iloc[:,i*max_features:(i+1)*max_features]\n",
    "        else:\n",
    "            ndf = df.iloc[:,i*max_features:df.shape[1]]\n",
    "        fig = ndf.boxplot(**kwargs)\n",
    "        fig.set_title(title + \" ({current}/{total})\".format(current=i+1, total=nplots) )\n",
    "        yield fig\n",
    "        \n",
    "def count_missing_nan(df:pd.DataFrame, axis:int=0):\n",
    "    '''\n",
    "        Return a new DataFrame with the counts of missing\n",
    "        and invalid values across specified axis.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "            - df:pandas.DataFrame := The data\n",
    "            \n",
    "            - axis:int=0 := The axis across which missing\n",
    "            values will be enumerated. Defaults to 0 (show\n",
    "            missing values in each column)\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        \n",
    "            - missing_df:pd.DataFrame := A DataFrame containing\n",
    "            counts of missing values\n",
    "    '''\n",
    "    ndf = pd.DataFrame(df.isna().sum(axis=axis)).T\n",
    "    ndf.index = ['']\n",
    "    return ndf\n",
    "\n",
    "\n",
    "class PlotPrior:\n",
    "    '''\n",
    "        WIP\n",
    "    '''\n",
    "    def __init__(self,data, columns:typing.Optional[list[str, tuple[str]]]=None,\n",
    "                 visible:bool=True, stack:bool=True, show_grid:bool=False, ncols:int=3,\n",
    "                kde:bool=True):\n",
    "        self.columns=columns\n",
    "        self.visible=visible\n",
    "        self.stack=stack\n",
    "        self.ncols=ncols\n",
    "        self.kde=kde\n",
    "        self.plots={} \n",
    "    \n",
    "    def __prep_data(self, data):\n",
    "        stacked = data.prior.stack(samples=[\"chain\", \"draw\"])\n",
    "        if self.columns is None:\n",
    "            self.columns = data.columns\n",
    "        for col in self.columns:\n",
    "            self.plots[col] = pd.DataFrame(data=stacked[col].values.T)\n",
    "    \n",
    "    def __call__(self, data, var:str):\n",
    "        df = self.plots[var]\n",
    "        k = df.shape[1]\n",
    "        n = self.ncols\n",
    "        m = (k - 1) // n + 1\n",
    "        fig, axes = plt.subplots(nrows=m, ncols=n, figsize=(n * 5, m * 3))\n",
    "        for i, (name, col) in enumerate(self.plots[var].items()):\n",
    "            r, c = i // n, i % n\n",
    "            ax = axes[r, c]\n",
    "            col.hist(ax=ax)\n",
    "            if self.kde:\n",
    "                ax2 = col.plot.kde(ax=ax, secondary_y=True, title=name)\n",
    "                ax2.set_ylim(0)\n",
    "        fig.tight_layout()\n",
    "        return fig,axs\n",
    "    \n",
    "def plot_kernel():\n",
    "    '''\n",
    "        Utility method for visualizing GP kernels. Produces a square visualization\n",
    "        of the selected kernel and all possible combinations of hyperparameters\n",
    "        \n",
    "        TODO\n",
    "        +++++\n",
    "        \n",
    "            - Extend this for combination kernels by nesting input hyperparameter\n",
    "            dicts\n",
    "            \n",
    "            - Raise combinatronic explsion error when too many hyperparameter values\n",
    "            are specified\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        \n",
    "            - \n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        \n",
    "            -fig:matplotlib.pyplot.Figure := The generated figure\n",
    "    '''\n",
    "    \n",
    "class MultiLayerPerceptronKernel(pymc.gp.cov.Covariance):\n",
    "    '''\n",
    "        Multi layer Perceptron Kernel a.k.a Arcsine Kernel\n",
    "        Code transplanted from `GPy.kern.src.mlp`. This kernel\n",
    "        is designed to mimic the highly nonlinear processes \n",
    "        of a Neural Network and can be thought of as the limit\n",
    "        of an A.N.N. with a single input layer as the number\n",
    "        of neurons in the hidden layer goes to infinity. Then\n",
    "        the N.N., with normal priors over the weights and biases\n",
    "        becomes equivalent to a G.P. This Kernel is given by:\n",
    "        \n",
    "        .. math::\n",
    "\n",
    "          k(x,y) = \\\\sigma^{2}\\\\frac{2}{\\\\pi }  \\\\text{asin} \\\\left\n",
    "          ( \\\\frac{ \\\\sigma_w^2 x^\\\\top y+\\\\sigma_b^2}{\\\\sqrt{\\\\sigma_w^2x^\\\\top x +\n",
    "          \\\\sigma_b^2 + 1}\\\\sqrt{\\\\sigma_w^2 y^\\\\top y + \\\\sigma_b^2 +1}} \\\\right )\n",
    "          \n",
    "        Args:\n",
    "        -----\n",
    "\n",
    "            - input_dims:int=1 := The number of input dimentions (columns) used for Covariance\n",
    "            computations. Defaults to 1 (use the first column only).\n",
    "\n",
    "            - active_dims:Optional[list[int]]=None := None or array-like specifying which input\n",
    "            dimentions will be used in computing the covariance matrix. Can be specified as a\n",
    "            a boolean vector or a vector of indices. Optional. Defaults to None, and the first\n",
    "            `input_dims` columns are selected.\n",
    "\n",
    "            - variance:Optional[float] := The output kernel variance scaling parameter, usually\n",
    "            denoted `\\eta`. Defaults to 1.0. Must be positive.\n",
    "\n",
    "            - weight_variance:Optional[Union[float, np.typing.NDArray[float]]]=1.0. The variance\n",
    "            of the weight in the A.N.N. Can be specified as either a scalar or a matrix of\n",
    "            appropriate size (same as the first input matrix). Optional.  Defaults to 1.0.\n",
    "\n",
    "            - bias_variance:Optional[Union[float, np.typing.NDArray[float]]]=1.0. The variance of\n",
    "            the biases in the A.N.N. Can be either a scalar or a matrix of appropriate size. When\n",
    "            computing covariance matrices (single input) must be a N-length vector of biases for a\n",
    "            `N \\times M` input matrix. When computing cross-covariance matrices, must be an\n",
    "            `N \\times N` matrix. Optional.  Defaults to 1.0.\n",
    "\n",
    "            - ARD:bool=False := A(utomatic)R(elevance)D(etermination) flag. Unused and raises\n",
    "            an error if switched. Optional. Defaults to False.\n",
    "        \n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        \n",
    "            - K:np.typing.NDArray := When called with the `.full(X)` or the `.full(X, Y)` method,\n",
    "            computes and returns the covariance or cross-covariance matrices.\n",
    "            \n",
    "            - diag(K):typing.NDArray := When called with the `.diag(X)` or `.diag(X, Y)` method\n",
    "            computes and returns the diagonal of the covariance or cross-covariance matrices\n",
    "    '''\n",
    "    four_over_tau = 2./np.pi\n",
    "    \n",
    "    def __init__(self,input_dims:int, active_dims:typing.Union[list[int], int]=None,\n",
    "                 variance:float=1.0,\n",
    "                 weight_variance:typing.Union[float, np.typing.NDArray]=1.0,\n",
    "                 bias_variance:typing.Union[float, np.typing.NDArray]=1.0, ARD:bool=False):\n",
    "        super(MultiLayerPerceptronKernel, self).__init__(input_dims, active_dims=active_dims)\n",
    "        self.variance = variance\n",
    "        self.weight_variance = weight_variance\n",
    "        self.bias_variance = bias_variance\n",
    "        if ARD:\n",
    "            raise NotImplementedError(\"Automatic Relevance Determination, not implemented\")\n",
    "        self.ARD = ARD\n",
    "\n",
    "    \n",
    "    def _comp_prod(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            return (at.math.square(X)*self.weight_variance).sum(axis=1)+self.bias_variance\n",
    "        else:\n",
    "            return (X*self.weight_variance).dot(X2.T)+self.bias_variance\n",
    "    \n",
    "    def diag(self, X):\n",
    "        \"\"\"Compute the diagonal of the covariance matrix for X.\"\"\"\n",
    "        X_prod = self._comp_prod(X)\n",
    "        return self.variance*MultiLayerPerceptronKernel.four_over_tau*at.math.arcsin(X_prod/(X_prod+1.))\n",
    "    \n",
    "    def full(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            X_denom = at.math.sqrt(self._comp_prod(X)+1.)\n",
    "            X2_denom = X_denom\n",
    "            X2 = X\n",
    "        else:\n",
    "            X_denom = at.math.sqrt(self._comp_prod(X)+1.)\n",
    "            X2_denom = at.math.sqrt(self._comp_prod(X2)+1.)\n",
    "        XTX = self._comp_prod(X,X2)/X_denom[:,None]/X2_denom[None,:]\n",
    "        return self.variance*MultiLayerPerceptronKernel.four_over_tau*at.math.arcsin(XTX)\n",
    "    \n",
    "def advi_inspect_ELBO(fit_data, show:bool=True):\n",
    "    '''\n",
    "        Utility method to investigate ADVI fit.\n",
    "        Returns log-ELBO against iterations.\n",
    "    '''\n",
    "    _h_advi_hist = fit_data.hist\n",
    "    advi_elbo = pd.DataFrame(\n",
    "        {'$log-ELBO$': -np.log(_h_advi_hist),\n",
    "         'n': np.arange(_h_advi_hist.shape[0])})\n",
    "    fig = sns.lineplot(y='$log-ELBO$', x='n', data=advi_elbo)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return fig\n",
    "\n",
    "def select_subarray(df:pd.DataFrame, targets:list[tuple[str,str,str]], indicators:list[tuple[str,str,str]],\n",
    "                   scaleX:bool=True, train_split:bool=False,\n",
    "                    mappings:typing.Optional[dict[str, typing.Union[str,int, float]]]=None,\n",
    "                    dummify:bool=False,\n",
    "                   )->tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    X = df.loc[:,indicators]\n",
    "    X = std_scale(X) if scaleX else X\n",
    "    Y = df.loc[:, targets]\n",
    "    m = Y.loc[~Y.isna().any(axis=1),:].index.intersection(X.loc[~X.isna().any(axis=1),:].index)\n",
    "    X = X.loc[m, :]\n",
    "    Y = Y.loc[m,:]\n",
    "    Y=Y.replace(mappings) if mappings is not None else Y\n",
    "    Y = pd.get_dummies(Y) if dummify else Y\n",
    "    if train_split:\n",
    "        X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y,test_size=.1 ,random_state=44)\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    else:\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c054b6b-5795-43db-aaa9-dcb6923c51c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_model(Xtrain, Xtest, Ytrain, Ytest, loss:str='categorical_crossentropy',\n",
    "             optimizer:str='adadelta',\n",
    "             layers:typing.Optional[list[tuple[int, str]]]=None,\n",
    "             metrics=['accuracy'], summary:bool=True, epochs:int=10,\n",
    "             factivation:str='sigmoid',batch:int=64, callbacks:list=[],\n",
    "            verbosity:int=1):\n",
    "    M = Xtrain.shape[1]\n",
    "    input_layer = tf.keras.Input(shape=(M,))\n",
    "    x = input_layer\n",
    "    for neurons, activation in layers:\n",
    "        x = tf.keras.layers.Dense(math.ceil(neurons), activation=activation)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(math.ceil(Ytrain.shape[1]), activation=factivation)(x)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=x)\n",
    "    model.compile(optimizer=optimizer, loss=loss, \n",
    "                metrics=metrics)\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    model.fit(Xtrain, Ytrain, epochs=epochs, validation_data = (Xtest, Ytest),\n",
    "              batch_size=batch, callbacks=callbacks, verbose=verbosity)\n",
    "    return model\n",
    "# Multiple query utility\n",
    "query_multiple = lambda df, col, multiquery : functools.reduce(lambda p,n: \"{col} == '{p}' | \".format(p=p, col=col) + \"{col} == '{n}\".format(n=n, col=col), multiquery).split(\" == '\",1)[1]+\"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac27bf5c-cafa-4210-a2cc-72d26452cffb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open('core_mappings.pickle', 'rb') as handle:\n",
    "        core_mappings = pickle.load(handle)\n",
    "    with open('r_core_mappings.pickle', 'rb') as handle:\n",
    "        r_core_mappings = pickle.load(handle)\n",
    "    df = pd.read_excel('evoos_processed.xlsx',header=2)\n",
    "    return core_mappings,r_core_mappings,df\n",
    "core_mappings, r_core_mappings, df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0a16a-9098-460c-918b-229903663813",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Διερεύνηση Διαφοροποίησης Χημικού Προφίλ Βάση Ωρίμανσης"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afaacfa-db3a-4a71-81aa-ce2312b6c84e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25,9))\n",
    "sns.heatmap(pd.concat([Y,X], axis=1).corr(method='kendall').iloc[Y.shape[1]:,:Y.shape[1]].applymap(np.abs),\n",
    "            cmap='viridis', ax=ax, annot=True)\n",
    "ax.set_title(\"Kendal Correlations Between Chemical Predictors and Continuous Production Targets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1e1ab-4ab5-40ba-a874-3c74e78becde",
   "metadata": {},
   "source": [
    "Αφού υπάρχει το επίμονο πρόβλημα οτι το Ν.Δ. προβλέπει τις παραμέτρους με τις μέσες τιμές (δηλαδή δεν αξιοποιεί κάποια χημική πληροφορία αλλά μαντεύει με τη μέση ή διάμεση) εξαιτάζουμε τις συσχετίσεις Kendall μεταξύ των παραμέτρων ενδιαφέροντως. Στη περιοχή των συνεχών μεταβλητών, παρατηρούμε κάποιες ασθενείς αλλά υπαρχείς συσχετίσεις. Για παράδειγμα υπάρχει μια συσχέτιση θερμοκρασίας μάλαξης και συγκέντρωσης λινολεϊκού $\\approxeq 20\\%$, ασθενής αλλά μη-αμεληταία. Η μέση ακρίβεια μας για την ποικιλία, είναι της τάξεως του $\\approxeq 70\\%$ και χρειάζεται βελτίωση. Παρατηρούμε οτι στο πίνακα συσχετίσεων Kendall φαίνεται να υπάρχει διαφοροποίηση μόνο στη περίπτωση \"ΠΑ\" και \"Μ\" ωρίμανσης, ενώ για τις περιπτώσεις \"ΠΙ\" και \"Π\" πρακτικά καμία συσχέτιση με το χημικό προφίλ. Ενδέχεται η παρουσία αυτών των κατηγοριών να μειώνει την ακρίβεια"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95e84e-5258-417f-bd75-04fd6738d0d8",
   "metadata": {},
   "source": [
    "Αρκετές διαδικασίες στατικής συμπερασματολογίας αφορούν τη σύγκριση μεταξύ ομάδων. Συχνά ενδιαφερόμαστε για το αν δύο σύνολα, που αντιπροσωπεύουν ομάδες είναι διαφορετικά μεταξύ τους ή το ένα μεγαλύτερο από το άλλο. Χρειάζεται κατάλληλο στατιστικό μοντέλο για να απαντηθούν τέτοιου είδους ερωτήματα γιατί οι όποιες διαφορές συνοδεύονται συνήθως από στοχαστικό θόρυβο μπορεί να οδηγήσει σε παραπλανητικά συμπεράσματα για τις παρατηρούμενες διαφορές.\n",
    "\n",
    "Η *de facto* μέθοδος αντιμετόπισης τέτοιων ερωτημάτων εμπλέκει κάποια στατιστική δοκιμασία. Δηλαδή εκφράζεται μία **μηδενική υπόθεση (*null hypothesis*)**, που συνήθως είναι η απουσία διαφοροποίησης μεταξύ των ομάδων, και τη χρήση κάποια **τιμή δοκιμής (*test statistic*)** για να καθοριστεί το εάν η παρατηρούμενη κατανομή των δεδομένων είναι αληθοφανής υπό τη μηδενική υπόθεση. Όταν η τιμή της δοκιμής υπερβαίνων κάποιο κατώφλι η μηδενική υπόθεση απορρίπτεται.\n",
    "\n",
    "Δυστυχώς, είναι γενικά δύσκολο να γίνουν στατιστικές υποθέσεις σωστά και τα πορίσματα τους εύκολο παρανοούνται. Ο καθορισμός στατιστικής δοκιμασίας εμπλέκει διάφορες υποκειμενικές επιλογές, όπως:\n",
    "\n",
    "* η στατιστική δοκιμή προς χρήση\n",
    "* η μηδενική υπόθεση\n",
    "* η στάθμη εμπιστοσύνης\n",
    "\n",
    "από τον χρήστη, που σπάνια τεκμηριώνονται από το εκάστοτε πρόβλημα, αλλά βασίζονται περισσότερο σε αυθαίρετες επιλογές και στατιστική παράδωση (*Johnson, 1999*). Οι αποδείξεις που παρέχουν στον χρήστη είναι έμμεσες, ατελείς και τυπικά υπερεκτιμούν τις αποδείξης κατά της μηδενικής υποθέσεως (*Goodman, 1999*).\n",
    "\n",
    "Μια πιο πληροφοριακή και αποτελεσματική προσέγγιση για συγκρίσεις ομάδων βασίζεται σε **εκτίμηση** έναντι **δοκιμής** και ωθείται από Μπεϋζιανή αντί frequentist πιθανότητα. Αντί να δοκιμάζεται το κατά πόσον δύο (ή περισσότερες) ομάδες διαφέρουν, **εκτιμούμε** τη διαφορά τους, κάτι που συνοδεύεται από εκτίμηση αβεβαιώτητας **επιστημική αβεβαιότητα (*epistemic uncertainty*)** και αβεβαιότητα λόγο έμφυτης στοχαστικότητας του συστήματος **κυβευτική αβεβαιότητα (*aleatory uncertainty*)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba81b4-6372-4e7b-a3f0-3a1ff0da9e6f",
   "metadata": {},
   "source": [
    "Θα εξαιτάσουμε αρχικά την απλούστερη περίπτωση, δύο ομάδων που αφορά τη μεταβλητή \"προσθήκη νερού\". Το πρώτο βήμα στη Μπεϋζιανή συμπερασματολογία είναι ο ορισμός του πλήρους μοντέλου πιθανοτητων επί του προβλήματος. Επειδή έχουμε 63 χημικές παραμέτρους ενδιαφέροντως θα επιλέξουμε πολυδιάστατη κατανομή. Επιλέγουμε την κατανομή Τ, γιατί είναι περισσότερο ανεκτική σε \"ακραίες\" τιμές. Η κατανομή Τ σε μία διάσταση καθορίζεται με τρείς παραμέτρους $\\mathcal{T(\\mu, \\lambda,\\nu)}$, ως εξής\n",
    "$$\n",
    "f(x|\\mu,\\lambda,\\nu)\\ =\\ \\dfrac{\\Gamma (\\dfrac{\\nu+1}{2})}{\\Gamma(\\dfrac\\nu2)}\\Bigg(\\dfrac{\\lambda}{\\pi\\nu}\\Bigg)\\Bigg[1+\\dfrac{\\lambda(x-\\mu)^2}\\nu\\Bigg]^{^{-\\dfrac{\\nu+1}{2}}}\n",
    "$$\n",
    "\n",
    "Η παράμετρος $\\mu$ αντιπροσωπεύει τον μέσο όρο, η παράμετρος $\\lambda$ την ακρίβεια ή αντίστοφη διακύμανση και η παράμετρος $\\nu$ τους βαθμούς ελευθερίας. Οι βαθμοί ελευθερίας αντιπροσωπεύουν την \"κανονικότητα\" της κατανομής $(\\nu\\rightarrow +\\infty)[\\mathcal{T}\\rightarrow\\mathcal{N}]$ και κατανομή τείνει στην κανονική, ενώ τιμές κοντά στο μηδέν δείνουν χοντρές ουρές."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cefbb1-8f2b-40fe-97a6-fd3eea0f17a4",
   "metadata": {},
   "source": [
    "Ορίζουμε την πιθανοφάνεια του μοντέλου ως εξής:\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\mathbf{x}^{\\text{Group 1}}_i\\thicksim\\ \\mathcal{T(\\nu,\\mu_1,\\sigma_1)}\\\\\n",
    "\\mathbf{x}^{\\text{Group 2}}_i\\thicksim\\ \\mathcal{T(\\nu,\\mu_2,\\sigma_2)}\\\\\n",
    "\\mathbf{x}^{\\text{Group 3}}_i\\thicksim\\ \\mathcal{T(\\nu,\\mu_3,\\sigma_3)}\\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{x}^{\\text{Group G}}_i\\thicksim\\ \\mathcal{T(\\nu,\\mu_G,\\sigma_G)}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ca601-8511-4633-9865-ea364de9eef1",
   "metadata": {},
   "source": [
    "Χάρην απλότητας υποτέθηκαν ίδιοι βαθμοί ελευθερίας αν και η υπόθεση δεν είναι απαραίτητη. Πρέπει να θέσουμε εκ των προτέρων κατανομές για τις άγνωστες παραμέτρους $\\mathbf{\\theta}=(\\mu_1,\\sigma_1,\\dots,\\mu_G,\\sigma_G)$. Η επιλογή μας αυτή καθορίζεται από την όποια εκ των προτέρων γνώση μας. Για παράδειγμα, εάν γνωρίζουμε οτι η προσθήκη νερού αυξάνει το ελαϊκό οξύ, από βιβλιογραφική ανασκόπηση, προηγούμενες έρευνες κλπ, μπορούν να ενσωματώσουμε αυτή τη πληροφορία στο μοντέλο μας επιλέγοντας κατάλληλες κατανομές πχ\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\mu_0\\thicksim\\mathcal{N(\\alpha,2\\sigma)}\\\\\n",
    "\\mu_1\\thicksim\\mathcal{N(10\\alpha,2\\sigma)}\\\\\n",
    "\\alpha\\in\\mathbb{R}^+\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Δεν έχουμε κάποια τέτοια ιδιαίτερη γνώση, οπότε θα υποθέσουμε κανονική κατανομή μέσου, τον συνενομένο (*pooled*) μέσο των δεδομένων. Ομοίως και για την τυπική απόκλιση.\n",
    "$$\n",
    "\\mu_G\\thicksim\\mathcal{N(\\bar{x},2\\sigma)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fee629-88ae-4b2d-84b2-84a343701868",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "means=pd.DataFrame(Xraw.mean())\n",
    "means.columns=[\"$\\mu$\"]\n",
    "stds=pd.DataFrame(Xraw.std())\n",
    "stds.columns=[\"$\\sigma$\"]\n",
    "gstats=pd.concat([means,stds],axis=1)\n",
    "gstats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2cf16-16cd-4cdf-acdf-88b97696657a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Παρατηρούμε κατα περίπτωση τεράστειες τιμές διακύμασης. Είναι πιθανό να οδηγηθούμε σε εξαιρετικά δύσκολη εκ των υστέρων κατανομή με αυτές τις τιμές. Ίσως να έχει νόημα να εξαιταστούν ορισμένες χωριστά. Θέτουμε ομοιόμορφες εκ των προτέρων για τις διακυμάνσεις:\n",
    "$$\n",
    "\\sigma_G\\thicksim\\mathcal{U(0.5,1000)}\n",
    "$$\n",
    "Αρχικά ας εξαιτάσουμε την απλή περίπτωση, διαφοροποίησης του λινολεϊκού οξέως ως προς τη προσθήκη νερού"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228f4ed-8c7b-481a-8212-19168d56b366",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepping data\n",
    "yeswater = Y.query(\"watadd_b == 1.0\")\n",
    "nowater = Y.query(\"watadd_b == .0\")\n",
    "X_water=Xraw.loc[Xraw.index.intersection(yeswater.index),[\"chemical.f_acids.lino\"]]\n",
    "X_no_water=Xraw.loc[Xraw.index.intersection(nowater.index),[\"chemical.f_acids.lino\"]]\n",
    "m1,m2 = X_water.mean().values[0], X_no_water.mean().values[0]\n",
    "sd1, sd2 = X_water.std(ddof=1).values[0], X_no_water.std(ddof=1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd7bd1-4c01-465b-a1a6-9f631d2f3857",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pymc.Model() as simple_linoleic_model:\n",
    "    μ1 = pymc.Normal('μ1', mu = m1, sigma=2*sd1)\n",
    "    μ2 = pymc.Normal('μ2', mu=m2, sigma=2*sd2)\n",
    "    δμ = pymc.Deterministic('δμ', μ1-μ2)\n",
    "    σ1 = pymc.Uniform('σ1', lower=1,upper=10 )\n",
    "    σ2 = pymc.Uniform('σ2', lower=1,upper=10 )\n",
    "    ν = pymc.Uniform('ν', lower=10, upper=100)\n",
    "    y1 = pymc.StudentT('y1', mu=μ1, nu=ν , sigma=σ1, observed = X_water.values)\n",
    "    y2 = pymc.StudentT('y2', mu=μ2, nu=ν , sigma=σ2, observed=X_no_water.values)\n",
    "    idata = pymc.sample(1000, tune = 2000, chains=2, cores=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129d994-b814-4283-b829-04d20d7468f9",
   "metadata": {},
   "source": [
    "Γενικά, θα χρειαστεί να δουλέψουμε με πολλαπλούς δείκτες, συνεπώς θα χρειαστούμε πολυδιάστατες κατανομές, διαφορετικά θα πρέπει να προσαρμώσουμε ξεχωριστές μονοδιάστατες κατανομές για κάθε συνιστώσα, αγνοώντας τις μεταξύ τους σχέσεις. Οι πολυδιάσταστες κατανομές, αντικαθιστούν τη διακύμανση με **πίνακα διακύμανσης**. Για παράδειγμα, για την κανονική κατανομή:\n",
    "$$\n",
    "\\mathcal{N}(\\mu,\\sigma)\\ \\ \\text{Σε μία διάσταση}\\\\\n",
    "\\mathcal{N^n}\\mathbf{\\big(\\mu, \\Sigma\\big)}\\ \\ \\ \\text{Σε n διαστάσεις χρειαζόμαστε $n\\times n\\ $ πίνακα συνδιακυμάνσεων ($\\Sigma$)}\n",
    "$$\n",
    "Συνεπώς χρειαζόμαστε μέθοδο για να θέσουμε εκ των προτέρων κατανομές σε πίνακες διακυμάνσεων. Μια τέτοια ειδική κατανομή είναι η Wishart, η οποία όμως έχει ορισμένα προβληματικά χαρακτηριστικά. Μία καλύτερη προσέγγιση είναι η κατανομή **LKJCholesky**, κατανομή επί πινάκων συσχετίσεων:\n",
    "$$\n",
    "\\mathbf{\\Sigma}\\ \\thicksim \\mathcal{LKJCholesky}(n, \\eta, s)\\\\\n",
    "\\text{Έχει τρεις ελεύθερες υπερ-παραμέτρους:}\\\\\n",
    "n\\ \\mathrel{\\vcenter{:}}=\\ \\text{Οι διαστάσεις των $n\\times\\ n$ πινάκων συσχέτισης}\\\\\n",
    "\\eta\\ \\mathrel{\\vcenter{:}}=\\ \\text{Παράμετρος σχέματος της κατανομής επί των πινάκων. Για $\\eta=1$ η κατανομή είναι ομοιόμορφη ενώ, $\\eta\\rightarrow +\\infty$ προτιμούνται πίνακες με ασθενέστες συσχετίσεις στα στοιχεία εκτός διαγωνίου}\\\\\n",
    "s\\ \\mathrel{\\vcenter{:}}=\\ \\text{Οι κατανομή των τυπικών αποκλίσεων των πινάκων συνδιακύμανσης}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7cc4f-d0d4-46aa-b210-2438893bbfce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LKJ_show(eta, sd_dist, n):\n",
    "    with pymc.Model() as test_model:\n",
    "        sds = pymc.Normal.dist(sd_dist,1)\n",
    "        chol,corr,stds = pymc.LKJCholeskyCov(\"packed_L\", n=n, eta=eta, sd_dist=sds, compute_corr=True)\n",
    "        # L = pymc.expand_packed_triangular(2, chol)\n",
    "        Σ = pymc.Deterministic('Σ', chol.dot(chol.T))\n",
    "        samples = pymc.sample_prior_predictive(samples=1)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c76ae-5b6d-4cd8-b290-4e2ea1eccf8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = LKJ_show(1, 2.0, 100)\n",
    "fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(25,10))\n",
    "sns.heatmap(samples.prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[0,0])\n",
    "axs[0,0].set_title(\"$eta=1,\\ s\\thicksim\\mathcal{HN}(2.0)$\")\n",
    "sns.heatmap(LKJ_show(20, 2.0, 100).prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[0,1])\n",
    "axs[0,1].set_title(\"$eta=20,\\ s\\thicksim\\mathcal{HN}(2.0)$\")\n",
    "sns.heatmap(LKJ_show(100, 2.0, 100).prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[0,2])\n",
    "axs[0,2].set_title(\"$eta=100,\\ s\\thicksim\\mathcal{HN}(2.0)$\")\n",
    "sns.heatmap(LKJ_show(1, .1, 100).prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[1,0])\n",
    "axs[1,0].set_title(\"$eta=1,\\ s\\thicksim\\mathcal{HN}(0.1)$\")\n",
    "sns.heatmap(LKJ_show(20, 0.1, 100).prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[1,1])\n",
    "axs[1,1].set_title(\"$eta=20,\\ s\\thicksim\\mathcal{HN}(0.1)$\")\n",
    "sns.heatmap(LKJ_show(100, 0.1, 100).prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[1,2])\n",
    "axs[1,2].set_title(\"$eta=100,\\ s\\thicksim\\mathcal{HN}(0.1)$\")\n",
    "sns.heatmap(LKJ_show(1, 100, 100).prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[2,0])\n",
    "axs[2,0].set_title(\"$eta=1,\\ s\\thicksim\\mathcal{HN}(100)$\")\n",
    "sns.heatmap(LKJ_show(20, 100, 100).prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[2,1])\n",
    "axs[2,1].set_title(\"$eta=20,\\ s\\thicksim\\mathcal{HN}(100)$\")\n",
    "sns.heatmap(LKJ_show(100, 100, 100).prior.stack(samples=['chain', 'draw'])['packed_L_corr'][:,:,0], cmap='viridis', ax=axs[2,2])\n",
    "axs[2,2].set_title(\"$eta=100,\\ s\\thicksim\\mathcal{HN}(100)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f034f44-bacb-4454-af56-fa8ad8a8e452",
   "metadata": {},
   "source": [
    "Τελικά για καταλήγουμε σε μοντέλο:\n",
    "$$\n",
    "\\begin{array}{clc}\n",
    "\\mathbf{\\stackrel{N\\times M}{x_1} \\thicksim \\mathcal{T}(\\mu_1, \\Sigma_1, \\nu_1)}&\\\\\n",
    "\\vdots &\\\\\n",
    "\\mathbf{x_K \\thicksim \\mathcal{T}(\\mu_K, \\Sigma_K, \\nu_K)}&\\\\\n",
    "&\\mathbf{\\nu_1=\\dots=\\nu_K=\\nu}&\\\\\n",
    "&\\mathbf{\\nu\\thicksim }&\\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54589091-6a52-473e-9e58-9a6acb8d5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_no_colmeth = pd.concat([production_targets.iloc[:,:6],production_targets.iloc[:,12:] ], axis=1)\n",
    "# Drop all nans and discard the first column as it tracks nan. Discard centr column as it's been modeled previously\n",
    "production_no_colmeth_no_nan = production_no_colmeth[~production_no_colmeth.isna().any(axis=1)].iloc[:,1:].drop(['centr'], axis=1)\n",
    "production_no_colmeth_no_nan\n",
    "# Scale inputs\n",
    "X = tidy_multiindex(chem_indicators)\n",
    "Y = production_no_colmeth_no_nan\n",
    "# Select all samples with valid entries for both predictors and targets\n",
    "valid = Y.index.intersection(X.index)\n",
    "X, Y= X.loc[valid,:], Y.loc[valid,:]\n",
    "# Rearrange Y columns to group variables by problem type\n",
    "Y = pd.concat(\n",
    "    [Y.iloc[:, :4], Y.iloc[:,6:], Y.iloc[:,[4]], Y.iloc[:,[5]]\n",
    "    ],axis=1)\n",
    "# Illicit values recorded as 40-50. Replaced with their mean\n",
    "Y = Y.replace(dict(kbtemp={'40-50':45.0})).astype(float)\n",
    "# Keep this for later\n",
    "Xraw=X\n",
    "X = pd.DataFrame(data=sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1)).fit_transform(X),\n",
    "                columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745f681-aff9-40ef-8cc4-c68d1ec619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping data\n",
    "Xraw = std_scale(Xraw)\n",
    "yeswater = Y.query(\"watadd_b == 1.0\")\n",
    "nowater = Y.query(\"watadd_b == .0\")\n",
    "X_water=Xraw.loc[Xraw.index.intersection(yeswater.index),:]\n",
    "X_no_water=Xraw.loc[Xraw.index.intersection(nowater.index),:]\n",
    "m1,m2 = X_water.mean(), X_no_water.mean()\n",
    "sd1, sd2 = X_water.std(ddof=1), X_no_water.std(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa37809-4c66-444c-943d-a7dde0166a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {'vars':X_water.columns.tolist() }\n",
    "with pymc.Model(coords=coords) as water_addition_model:\n",
    "    μ_water = pymc.MvNormal('μ_water', mu = X_water.mean().values, cov = 1.5*X_water.cov().values, dims='vars')\n",
    "    μ_no_water = pymc.MvNormal('μ_no_water', X_no_water.mean().values, cov = 1.5*X_no_water.cov().values, dims='vars')\n",
    "    δμ = pymc.Deterministic('δμ', μ_water-μ_no_water, dims='vars')\n",
    "    ν_water= pymc.HalfCauchy('ν_water', 2.0)\n",
    "    ν_no_water = 1.0\n",
    "    η_water = 1\n",
    "    η_no_water = 1\n",
    "    sd_water = pymc.HalfCauchy.dist(2)\n",
    "    sd_no_water = pymc.HalfCauchy.dist(2)\n",
    "    L_water, corr_water, deviations_water =  pymc.LKJCholeskyCov('LKJ_water', n=X_water.shape[1], sd_dist=sd_water,eta=η_water, compute_corr=True)\n",
    "    L_no_water, corr_no_water, deviations_no_water = pymc.LKJCholeskyCov('LKJ_no_water', n=X_water.shape[1], sd_dist=sd_no_water, eta=η_no_water, compute_corr=True)\n",
    "    Σ_water, Σ_no_water = pymc.Deterministic(\"Σ_water\", L_water.dot(L_water.T)), pymc.Deterministic(\"Σ_no_water\", L_no_water.dot(L_no_water.T))\n",
    "    observed_water = pymc.MvStudentT('observed_water', mu=μ_water,nu=ν_water, scale=Σ_water, observed = X_water.values)\n",
    "    observed_no_water = pymc.MvStudentT('observed_no_water', mu=μ_no_water,nu=ν_water, scale=Σ_no_water, observed = X_no_water.values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab127841-7cd5-42db-9192-fd7270efcf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with water_addition_model:\n",
    "    prior_samples = pymc.sample_prior_predictive(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874319d1-5aee-4548-85a6-aa64036575f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepping data\n",
    "yeswater = Y.query(\"watadd_b == 1.0\")\n",
    "nowater = Y.query(\"watadd_b == .0\")\n",
    "X_water=Xraw.loc[Xraw.index.intersection(yeswater.index),:]\n",
    "X_no_water=Xraw.loc[Xraw.index.intersection(nowater.index),:]\n",
    "m1,m2 = X_water.mean(), X_no_water.mean()\n",
    "sd1, sd2 = X_water.std(ddof=1), X_no_water.std(ddof=1)\n",
    "\n",
    "class EstimateGroupDifference:\n",
    "    '''\n",
    "        Perform Bayesian estimation  of the differences between\n",
    "        groups. WIP\n",
    "    '''\n",
    "    def __init__(self, X:pd.DataFrame, Y:pd.DataFrame,\n",
    "                 target_indicators:list[str]):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.target_indicators = target_indicators\n",
    "        self.models = {k:None for k in target_indicators}\n",
    "        self.coords = {'indicator_variables':X.columns.tolist()}\n",
    "        \n",
    "    def select_subsets(self):\n",
    "        \n",
    "        for model in self.models.keys():\n",
    "            groups = self.Y.loc[:,model].unique()\n",
    "            self.models[model] = {\n",
    "                'ngroups':len(groups)\n",
    "            }\n",
    "            t = {}\n",
    "            for group in groups:\n",
    "                _y= self.Y.query(\"{varname} == {group}\".format(varname=model,\n",
    "                    group=group))\n",
    "                t[f'Y == {group}'] = {\n",
    "                        'Y_mask':_y.index,\n",
    "                        'X_mask':self.X.index.intersection(_y.index)\n",
    "                    }\n",
    "            self.models[model] = self.models[model] | t\n",
    "        return self.models\n",
    "    \n",
    "#     def build_models(self):\n",
    "        \n",
    "#         for model, model_specs in self.models.items():\n",
    "           \n",
    "#             with pymc.Model(coords=self.coords):\n",
    "#                 μ_water = pymc.MvNormal('μ_water', mu = X_water.mean().values, cov = 2*X_water.cov().values, dims='vars')\n",
    "#                 μ_no_water = pymc.MvNormal('μ_no_water', X_no_water.mean().values, cov = 2*X_no_water.cov().values, dims='vars')\n",
    "#                 δμ = pymc.Deterministic('δμ', μ_water-μ_no_water, dims='vars')\n",
    "#                 # ν_water = pymc.HalfCauchy('ν_water', 3.0)\n",
    "#                 # ν_no_water = pymc.HalfCauchy('ν_no_water',3.0)\n",
    "#                 ν_water= 1.0\n",
    "#                 ν_no_water = 1.0\n",
    "#                 η_water = 1.0\n",
    "#                 η_no_water = 1.0\n",
    "#                 sd_water = pymc.HalfCauchy.dist(5.0,shape=63)\n",
    "#                 sd_no_water = pymc.HalfCauchy.dist(5.0)\n",
    "#                 Lpacked1,cor1,std1 =  pymc.LKJCholeskyCov('LKJ1', n=X_water.shape[1], sd_dist=sd_no_water,eta=η_water, compute_corr=True)\n",
    "#                 Lpacked2,cor2,std2 = pymc.LKJCholeskyCov('LKJ2', n=X_water.shape[1], sd_dist=sd_no_water, eta=η_no_water, compute_corr=True)\n",
    "#                 L_water = pymc.Deterministic('L_water', Lpacked1.dot(Lpacked1.T))\n",
    "#                 L_no_water = pymc.Deterministic('L_no_water', Lpacked2.dot(Lpacked2.T))\n",
    "#                 observed_water = pymc.MvStudentT('observed_water', mu=μ_water,nu=ν_water, cov=L_water, observed = X_water.values)\n",
    "#                 observed_no_water = pymc.MvStudentT('observed_no_water', mu=μ_no_water,nu=ν_no_water, cov=L_no_water, observed = X_no_water.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ce6c0-468c-45f1-85c7-27a514a63cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = prior_samples.prior.stack(samples=[\"chain\", \"draw\"])\n",
    "sns.heatmap(prior[\"LKJ_water_corr\"][:,:,0].values, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfa899-e3ba-4bfd-9be6-a69fc9d28e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with water_addition_model:\n",
    "    idata = pymc.sample(draws=1000, tune=1000, chains=3, cores=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb95c51-c722-438e-aa83-8d72bd95184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata.to_netcdf(\"water_addition_difference_estimation_model.nc\")\n",
    "# az.from_netcdf(\"water_addition_difference_estimation_model.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4083f-8f63-42ea-9bb3-93ca5f729b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mat=Y.loc[:, [\"mat_p\", \"mat_pa\", \"mat_pi\", \"mat_m\"]]\n",
    "varlist = Y.columns.tolist()\n",
    "# Build as many models as there are target\n",
    "# variables\n",
    "modeldict={k:None for k in varlist}\n",
    "for target_var in Y_mat.columns:\n",
    "    # For each model we fetch K-groups as distinct values of\n",
    "    # the target variable (a.k.a factors)\n",
    "    factors = Y.loc[:,target_var].unique()\n",
    "    modeldict[target_var] = dict(n_groups=len(factors), factors = factors)\n",
    "    for factor in factors:\n",
    "        # We split the X matrix to sub-matrices where Y has the factor\n",
    "        # value\n",
    "        # Samples with the factor\n",
    "        y_index = Y_mat.query(f\"{target_var} == {factor}\").index\n",
    "        # Corresponding samples in X matrix, used to sub-samble\n",
    "        x_index = Xraw.index.intersection(y_index)\n",
    "        factordict={str(factor):{\n",
    "            'X':x_index,\n",
    "            'Y':y_index,\n",
    "            }\n",
    "                   }\n",
    "        modeldict[target_var] = modeldict[target_var]|factordict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edac9a-be24-4fb4-a801-524bbb5d5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw =Xraw\n",
    "coords = {'vars':X.columns.tolist() }\n",
    "idx1 = Y.query(\"mat_p == 1.0\").index\n",
    "idx2 = Y.query(\"mat_p == 0.0\").index\n",
    "xidx1 = X_raw.index.intersection(idx1)\n",
    "xidx2 = X_raw.index.intersection(idx2)\n",
    "mu1 = X_raw.loc[xidx1,:].mean() \n",
    "mu2 =X_raw.loc[xidx2,:].mean() \n",
    "sdev1 =X_raw.loc[xidx1,:].cov()\n",
    "sdev2 =X_raw.loc[xidx2,:].cov()\n",
    "X_raw = (X_raw-X_raw.mean())/X_raw.std(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13406959-ab55-4da8-82b0-2b032dd620db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pymc.Model(coords=coords) as maturation_p_model:\n",
    "    μ1 = pymc.Normal('μ1', mu = 0.00, sigma= 1, size=X_raw.shape[1], dims='vars')\n",
    "    μ2 = pymc.Normal('μ2', mu = 0.00, sigma= 1, size=X_raw.shape[1], dims='vars')\n",
    "    δμ12 = pymc.Deterministic('δμ12', μ1-μ2, dims='vars')\n",
    "    β1 = pymc.Beta('β1', alpha=5.0, beta=1.0)\n",
    "    ν1 = pymc.Deterministic('ν1', 10.0*β1)\n",
    "    ν2 = pymc.Deterministic('ν2', 10.0*β1)\n",
    "    η1 = 10.0\n",
    "    η2 = 10.0\n",
    "    sd1 = pymc.Exponential.dist(1)\n",
    "    sd2 = pymc.Exponential.dist(1)\n",
    "    L1, corr1, deviations1 =  pymc.LKJCholeskyCov('LKJ1', n=X_raw.shape[1], sd_dist=sd1,eta=η1, compute_corr=True)\n",
    "    L2, corr2, deviations2 = pymc.LKJCholeskyCov('LKJ2', n=X_raw.shape[1], sd_dist=sd2, eta=η2, compute_corr=True)\n",
    "    Σ1, Σ2 = pymc.Deterministic(\"Σ1\", L1.dot(L1.T)), pymc.Deterministic(\"Σ2\", L2.dot(L2.T))\n",
    "    observed_1 = pymc.MvStudentT('observed_1', mu=μ1,nu=ν1, scale=Σ1, observed = X_raw.loc[xidx1,:].values)\n",
    "    observed_2 = pymc.MvStudentT('observed_2', mu=μ2,nu=ν2, scale=Σ2, observed = X_raw.loc[xidx2,:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52a1e7-2dc3-43a3-ae54-5882f320fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with maturation_p_model:\n",
    "    pdata = pymc.sample_prior_predictive(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e0443-b7c3-44a0-a959-9fbd7f3a77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = pdata.prior.stack(samples=['chain', 'draw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dae089-13bb-4631-87f9-7c8c18cff391",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior['μ1'].T.to_pandas().reset_index().iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb1720-f06b-495e-99cc-e62040ab4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = ['μ1', 'μ2']\n",
    "fig, axs = plt.subplots(figsize=(25,10), ncols=1, nrows=len(plots))\n",
    "for i, plot in enumerate(plots):\n",
    "    _x = prior[plot].T.to_pandas().reset_index().iloc[:,2:]\n",
    "    _x_=_x.melt()\n",
    "    axs[i].set_title(f\"Prior Mean Distributions, Group {i}\")\n",
    "    sns.kdeplot(_x_, x=\"value\", hue=\"vars\",ax=axs[i],fill=True, palette=\"viridis\",\n",
    "       alpha=.5, linewidth=0, cut=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9fcae-3f66-4f6d-8587-6d1064356947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3b602-282f-4009-9099-6907f3c9fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with maturation_p_model:\n",
    "    idata = pymc.sample(1000, tune=2000, chains=2, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a02d8e-054f-4e89-bfdb-05270fb70f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata.to_netcdf(\"maturation_first_attempt.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc30e7-a354-4bca-a414-f2aac5c8b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pymc.Model(coords=coords) as maturation_p_model:\n",
    "    μ1 = pymc.MvNormal('μ1', mu = X_raw[xidx1,:].mean().values, cov=100*X_raw[xidx1,:].cov().values, dims='vars')\n",
    "    μ2 = pymc.MvNormal('μ2', mu = 0.00, sigma= 1, size=X_raw.shape[1], dims='vars')\n",
    "    δμ12 = pymc.Deterministic('δμ12', μ1-μ2, dims='vars')\n",
    "    β1 = pymc.Beta('β1', alpha=5.0, beta=1.0)\n",
    "    ν1 = pymc.Deterministic('ν1', 10.0*β1)\n",
    "    ν2 = pymc.Deterministic('ν2', 10.0*β1)\n",
    "    η1 = 10.0\n",
    "    η2 = 10.0\n",
    "    sd1 = pymc.Exponential.dist(1)\n",
    "    sd2 = pymc.Exponential.dist(1)\n",
    "    L1, corr1, deviations1 =  pymc.LKJCholeskyCov('LKJ1', n=X_raw.shape[1], sd_dist=sd1,eta=η1, compute_corr=True)\n",
    "    L2, corr2, deviations2 = pymc.LKJCholeskyCov('LKJ2', n=X_raw.shape[1], sd_dist=sd2, eta=η2, compute_corr=True)\n",
    "    Σ1, Σ2 = pymc.Deterministic(\"Σ1\", L1.dot(L1.T)), pymc.Deterministic(\"Σ2\", L2.dot(L2.T))\n",
    "    observed_1 = pymc.MvStudentT('observed_1', mu=μ1,nu=ν1, scale=Σ1, observed = X_raw.loc[xidx1,:].values)\n",
    "    observed_2 = pymc.MvStudentT('observed_2', mu=μ2,nu=ν2, scale=Σ2, observed = X_raw.loc[xidx2,:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c862a37-503b-4977-a66f-7543abf56912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evoos_env",
   "language": "python",
   "name": "evoos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
